[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Noah P",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Design and Impact of Automated Decision Systems\n\n\n\n\n\nNoah Price Blog Post 2 - CSCI0451\n\n\n\n\n\nMar 5, 2025\n\n\nNoah Price\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\nNoah Price Blog Post 1 - CSCI0451\n\n\n\n\n\nFeb 21, 2025\n\n\nNoah Price\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nTimnit Gebru\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/blog-post-1-penguins/index.html",
    "href": "posts/blog-post-1-penguins/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "The Palmer Penguins dataset provides a set of physical characteristics of penguins of three different species: Adélie, Chinstrap, and Gentoo. Using all the physical characteristics to train a model would invariably yield highly accurate results, however, there may exist strong enough trends in the dataset to support a highly accurate model using a smaller number of features. This analysis will cover an examination of the Palmer Penguins dataset, as well as a generalizable method for determining the three strongest features for classifying the penguins. Discussion will go into strengths and weaknesses of various models and features, as well as justifying why the strongest performing model was best suited to the task at hand."
  },
  {
    "objectID": "posts/blog-post-1-penguins/index.html#abstract",
    "href": "posts/blog-post-1-penguins/index.html#abstract",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "The Palmer Penguins dataset provides a set of physical characteristics of penguins of three different species: Adélie, Chinstrap, and Gentoo. Using all the physical characteristics to train a model would invariably yield highly accurate results, however, there may exist strong enough trends in the dataset to support a highly accurate model using a smaller number of features. This analysis will cover an examination of the Palmer Penguins dataset, as well as a generalizable method for determining the three strongest features for classifying the penguins. Discussion will go into strengths and weaknesses of various models and features, as well as justifying why the strongest performing model was best suited to the task at hand."
  },
  {
    "objectID": "posts/blog-post-1-penguins/index.html#exploration",
    "href": "posts/blog-post-1-penguins/index.html#exploration",
    "title": "Classifying Palmer Penguins",
    "section": "Exploration",
    "text": "Exploration\nFirst, let’s access the training dataset and take a look at it.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\nIn order for our models to train effectively on these features, we’ll want to transform the columns to make them more machine-friendly. In particular, we will separate the species column from the rest of the data and encode species as integers (0 for Adélie, 1 for Chinstrap, and 2 for Gentoo). This will allow us to train on the data without the species labels, then check our work against the labels. We will also drop any rows with missing data so we only work with penguins for which we have complete data. Lastly, we will encode qualitative features such as the island the penguin was found on or the penguin’s sex as “one-hot columns”, meaning they will either be 1 for true or 0 for false.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df, dtype=int)\n  return df, y\n\nX_train, y_train = prepare_data(train)\nX_train\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\n0\n1\n0\n1\n0\n1\n1\n0\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\n0\n1\n0\n1\n0\n1\n0\n1\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\n1\n0\n0\n1\n0\n1\n0\n1\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\n1\n0\n0\n1\n0\n1\n1\n0\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\n0\n1\n0\n1\n0\n1\n0\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n270\n51.1\n16.5\n225.0\n5250.0\n8.20660\n-26.36863\n1\n0\n0\n1\n0\n1\n0\n1\n\n\n271\n35.9\n16.6\n190.0\n3050.0\n8.47781\n-26.07821\n0\n0\n1\n1\n1\n0\n1\n0\n\n\n272\n39.5\n17.8\n188.0\n3300.0\n9.66523\n-25.06020\n0\n1\n0\n1\n0\n1\n1\n0\n\n\n273\n36.7\n19.3\n193.0\n3450.0\n8.76651\n-25.32426\n0\n0\n1\n1\n0\n1\n1\n0\n\n\n274\n42.4\n17.3\n181.0\n3600.0\n9.35138\n-24.68790\n0\n1\n0\n1\n0\n1\n1\n0\n\n\n\n\n256 rows × 14 columns\n\n\n\nBefore getting into using models for classification, it will be helpful to examine some of the features to get some preliminary ideas of what good choices for features will be. For the purposes of visualization only, we will create a dataframe containing both the features and the species of the penguins, so that we can plot them against each other. The following code block will create this dataframe, and use it to generate our first data visualization.\n\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt \n\nspecies_mapping = {0: \"Adélie\", 1: \"Chinstrap\", 2: \"Gentoo\"}\nspecies = np.vectorize(species_mapping.get)(y_train) # Create array with species names\n\n# Create a copy of X_train which has the species names (used only for visualizations!)\nX_train_dv = X_train.copy()\nX_train_dv[\"Species\"] = species\n\nsns.scatterplot(\n    x=X_train_dv[\"Flipper Length (mm)\"],\n    y=X_train_dv[\"Body Mass (g)\"],\n    hue=X_train_dv[\"Species\"],\n    palette=\"Set1\"\n)\n\nplt.xlabel(\"Flipper Length (mm)\")\nplt.ylabel(\"Body Mass (g)\")\nplt.title(\"Fig. 1: Penguin Flipper Length and Body Mass by Species\")\nplt.legend(title=\"Species\")\nplt.show()\n\n\n\n\n\n\n\n\nFig. 1 plots the penguins based on their body mass on the y-axis, and flipper length on the x-axis. These qualitative features seemed like a good starting point, as, together, they provide a reasonable estimate of a penguin’s overall size and shape, which may be indicative of species. What’s notable about this figure is that there appear to be two ‘clusters’: one on the lower end of the chart with a fair mix of Adélie and Chinstrap penguins, and one on the higher end with almost all Gentoo penguins. What this suggests is that these quantitative features may not be sufficient on their own to distinguish between species, as Adélie and Chinstrap penguins sit in a cluster; however, these features would be highly effective for identifying Gentoo penguins, since they have a tendency to be larger than the other types.\n\nsns.scatterplot(\n    x=X_train_dv[\"Delta 15 N (o/oo)\"],\n    y=X_train_dv[\"Delta 13 C (o/oo)\"],\n    hue=X_train_dv[\"Species\"],\n    palette=\"Set1\"\n)\n\nplt.xlabel(\"Delta 15 N (o/oo)\")\nplt.ylabel(\"Delta 13 C (o/oo)\")\nplt.title(\"Fig. 2: Delta 13 C and Delta 15 N by Species\")\nplt.legend(title=\"Species\")\nplt.show()\n\n\n\n\n\n\n\n\nFig. 2 plots the penguins by species based on their Delta 13 C and Delta 15 N, which are measurements of the carbon isotopes and nitrogen isotopes present in the penguin’s blood. The ratios of these isotopes is related to the penguin’s diet, meaning it could be different between different species if they eat different types of food. The figure suggests a loose correlation within each group– the Gentoo penguins trend towards the bottom left, the Chinstrap penguins trend towards the top right, and the Adélie penguins are somewhere in the middle. However, it is notable that Adélie penguins also appear in the Chinstrap and Gentoo clusters, meaning this quantitative feature may be effective for identifying Chinstrap and Gentoo penguins, but poor for identifying Adélie penguins.\n\n# Compute mean and median for culmen length and depth\nsummary_table = X_train_dv.groupby(\"Species\").aggregate(\n    Mean_Culmen_Length_mm=(\"Culmen Length (mm)\", \"mean\"),\n    Median_Culmen_Length_mm=(\"Culmen Length (mm)\", \"median\"),\n    Mean_Culmen_Depth_mm=(\"Culmen Depth (mm)\", \"mean\"),\n    Median_Culmen_Depth_mm=(\"Culmen Depth (mm)\", \"median\")\n)\n\n# Display the table\nprint(summary_table)\n\n           Mean_Culmen_Length_mm  Median_Culmen_Length_mm  \\\nSpecies                                                     \nAdélie                 38.961111                    38.90   \nChinstrap              48.771429                    49.25   \nGentoo                 47.133696                    46.55   \n\n           Mean_Culmen_Depth_mm  Median_Culmen_Depth_mm  \nSpecies                                                  \nAdélie                18.380556                   18.50  \nChinstrap             18.346429                   18.25  \nGentoo                14.926087                   14.80  \n\n\nThis summary table shows the mean and median values of culmen length and culmen depth by species. Adélie penguins have low average culmen lengths, and Gentoo penguins have low average culmen depths. Chinstrap penguins have high scores on both. If only one of these features were used, it would be difficult to classify all three types, as two of the species are similar on each feature individually. However, when looking at both features at the same time, the pairs of scores seem to be strong potential indicators of species. Adélie penguins should have low length but high depth, Gentoos should have high length and low depth, and Chinstraps should have both. To finish our data examination, let’s now move to some qualitative features.\n\nisland_columns = [\"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"]\nX_train_dv[\"island\"] = X_train_dv[island_columns].idxmax(axis=1).str.replace(\"Island_\", \"\")\n\n# Count the number of penguins by island and species\nisland_species_counts = X_train_dv.groupby([\"island\", \"Species\"]).size().reset_index(name=\"count\")\n\n# Plot using seaborn barplot\nplt.figure(figsize=(8, 5))\nsns.barplot(x=\"island\", y=\"count\", hue=\"Species\", data=island_species_counts, palette=\"Set2\")\n\nplt.xlabel(\"Island\")\nplt.ylabel(\"Number of Penguins\")\nplt.title(\"Fig. 3: Penguin Species Counts by Island\")\nplt.legend(title=\"Species\")\nplt.show()\n\n\n\n\n\n\n\n\nFig. 3 displays the species counts by island. This chart holds some initially promising results– Gentoo penguins are only found on Biscoe Island, and Chinstrap penguins are only found on Dream island. However, there are also some problems, in that Adélie penguins are found on all three islands (though at different proportions), and, more subtly, this may suggests that the model could overfit based on these features. If, for example, the testing dataset contains a Gentoo or Chinstrap penguin found on Torgersen island, a model trained on this feature would likely predict that penguin to be Adélie (unless other features strongly suggested otherwise). Unless this subset of the dataset correctly shows that there are no Gentoo or Chinstrap penguins on Torgersen island, this pattern has the potential to yield inaccurate predictions.\n\nsex_columns = [\"Sex_FEMALE\", \"Sex_MALE\"]\nX_train_dv[\"sex\"] = X_train_dv[sex_columns].idxmax(axis=1).str.replace(\"Sex_\", \"\")\n\n# Count the number of penguins by island and species\nsex_counts = X_train_dv.groupby([\"sex\", \"Species\"]).size().reset_index(name=\"count\")\n\n# Plot using seaborn barplot\nplt.figure(figsize=(8, 5))\nsns.barplot(x=\"Species\", y=\"count\", hue=\"sex\", data=sex_counts, palette=\"Set2\")\n\nplt.xlabel(\"Species\")\nplt.ylabel(\"Number of Penguins\")\nplt.title(\"Fig. 4: Penguin Species Counts by Sex\")\nplt.legend(title=\"Sex\")\nplt.show()\n\n\n\n\n\n\n\n\nLastly, Fig. 4 shows the proportions of sex by species in the dataset. The expected rate of sex over a wide enough dataset may seem, intuitively, to be 50%, but factors such as differing behaviors between sexes may change the proportions depending on how different species operate. In this dataset, the counts within each species are relatively close, with Adélie penguins having more males than females, and the others having more females than males. Given the lack of significant variance within each species, it seems unlikely based on this visualization that sex would be a strong predictor of species."
  },
  {
    "objectID": "posts/blog-post-1-penguins/index.html#modeling",
    "href": "posts/blog-post-1-penguins/index.html#modeling",
    "title": "Classifying Palmer Penguins",
    "section": "Modeling",
    "text": "Modeling\nNow it’s time to select a model and a set of features. I experimented with several models, including the RandomForestClassifier, GaussianProcessClassifier, and Naive-Bayes classifier. All of these models performed decently, attaining maximum training accuracies in the 90+ percent range. However, what I found to be the most effective was the good old LogisticRegression. LogisticRegressions are highly effective when differences in data are linearly bound, while other models perform better on highly non-linear distinctions. Therefore, if our dataset shows linear distinctions between types, a LogisticRegression may outperform other models. Given our visualizations earlier, it was easy to see how, in some cases at least, lines could be drawn between clusters of data points of each species. This suggests that the LR model may be well suited to the task.\nIn terms of selecting features, the relatively modest size of the dataset allows a relatively modest search to get the job done. Simply iterating through every possible combination of two quantitative features and one qualitative feature (and keeping track of the best scoring features) will suffice in this case, though this is not a scalable method for larger datasets. The code block below implements this process, selecting two quantitative columns and one qualitative column in each iteration, then training a LogisticRegression using those features. The model is then evaluated using five-fold cross validation to ensure that it does not overfit to the training dataset. At the end, it will print what the best features were, as well as the training accuracy of the model.\nThe only other notable implementation here is the application of a scaler to the quantitative columns. LogisticRegressions work by finding minimum points of a function; by scaling quantitative data such that it follows a more normal distribution, this process can be carried out more efficiently and effectively. Therefore, as a final preprocessing step, we will apply a StandardScaler to the quantitative columns, which will help the LR converge in fewer iterations.\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\n\nqual_cols = [\"Clutch Completion\", \"Sex\", \"Island\", \"Stage_Adult, 1 Egg Stage\"]\nquant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\n\nscaler = StandardScaler() # Scaling quantitative values decreases the num. of iterations needed for LR\nX_train_scaled = X_train.copy()\nX_train_scaled[quant_cols] = scaler.fit_transform(X_train[quant_cols])\n\nbest_cols = []\nbest_score = 0\nfor qual in qual_cols: \n  qual_cols = [col for col in X_train_scaled.columns if qual in col ]\n  for pair in combinations(quant_cols, 2):\n    cols = qual_cols + list(pair) \n    LR = LogisticRegression()\n    LR.fit(X_train_scaled[cols], y_train)\n    scores = cross_val_score(LR, X_train_scaled[cols], y_train, cv=5) # Using 5-fold cross validation here\n    mean_score = np.mean(scores)\n    if (mean_score &gt; best_score):\n      best_cols = cols\n      best_score = mean_score\n\nprint(best_cols)\nprint(best_score)\n\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n0.9922322775263952\n\n\nThe best features for the model turned out to be culmen length and culmen depth for the quantitative columns, along with sex for the qualitative column. Culmen length and culmen depth make sense, as the summary table showed that the two, when combined, could function as a very strong identifier. Sex is an unexpected result for the qualitative column, as the visualizations suggested no strong correlation between sex and species. However, it is possible that sex scored the best because it avoided overfitting to the dataset. The use of cross-validation means that overfitting will lead to poor scores, so it is possible that the other qualitative columns showed inaccurate patterns in the training data. In this case, a feature like sex which has only weak correlations could perform better than ones with misleading patterns."
  },
  {
    "objectID": "posts/blog-post-1-penguins/index.html#evaluation",
    "href": "posts/blog-post-1-penguins/index.html#evaluation",
    "title": "Classifying Palmer Penguins",
    "section": "Evaluation",
    "text": "Evaluation\nThe iterative process yielded a training accuracy of 99.2%, which seems fairly solid. To properly evaluate it, though, we must compare it to the base rate. The following code block first uses np.bincount to count instances of each species in y_train, then computes the base rate by dividing the number of penguins in the most popular species by the total number of species. This represents the accuracy rate we would achieve if we always predicted that a penguin’s species is whatever the most popular species is.\n\ncounts = np.bincount(y_train)\nres = counts.max() / counts.sum()\nprint(res)\n\n0.421875\n\n\nThe base rate is 42.1%, so we have significantly outperformed the base rate. Now, let’s take the selected columns and evaluate the LR model on the test dataset, making sure to apply the same preprocessing steps to the test data.\n\ncols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Sex_FEMALE', 'Sex_MALE']\nLR = LogisticRegression()\nLR.fit(X_train_scaled[cols], y_train)\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\nX_test[quant_cols] = scaler.transform(X_test[quant_cols]) # Apply the same scaling to the test dataset!\n\nLR.score(X_test[cols], y_test)\n\n0.9852941176470589\n\n\nThe model achieves a 98.5% accuracy rate on the testing dataset, which suggests that it has generalized relatively effectively (meaning it has not overfitted to the training data set). The following code block plots the decision regions for the LR model.\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = \"Culmen Length (scaled)\", \n            ylabel  = \"Culmen Depth (scaled)\", \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\nplot_regions(LR, X_train_scaled[cols], y_train)\n\n\n\n\n\n\n\n\nThe decision regions of the LR appear, on visual inspection, to be highly accurate, with one misplaced point in the Female chart and one just on the line in the Male chart. These decision regions further elucidate why sex was chosen for the qualitative feature– it seems that males have generally larger culmens than females, which allows classifications made based on culmen length and depth to be even more accurate. To get a better idea of how the model performed on the test data, we will look at the confusion matrix. The code block below generates the confusion matrix by re-making the predictions, then comparing them to the actual test data.\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = LR.predict(X_test[cols])\nC = confusion_matrix(y_test, y_test_pred)\nC\n\narray([[31,  0,  0],\n       [ 0, 10,  1],\n       [ 0,  0, 26]])\n\n\nIn the confusion matrix above, the left column represents penguins that were classified as Adélie, the middle column represents penguins which were classified as Chinstrap, and the right column represents penguins that were classified as Gentoo. The first row were the Adélie penguins, the second row were the Chinstrap penguins, and the third row were the Gentoo penguins. What this means is that the only misclassification that the model made was classifying a single Chinstrap penguin as a Gentoo penguin. Otherwise, 31 Adélie penguins, 10 Chinstrap penguins, and 26 Gentoo penguins were correctly identified. Thinking back to the features, it makes sense that the error the model made was classifying a Chinstrap penguin as a Gentoo penguin, as the distinction between Gentoos and the other types in terms of culmen depth was a smaller distinction than the difference between, e.g. Adélie penguins and the others in terms of culmen length. If a Chinstrap penguin had a particularly low culmen depth, the model could have classified it as a Gentoo penguin."
  },
  {
    "objectID": "posts/blog-post-1-penguins/index.html#discussion",
    "href": "posts/blog-post-1-penguins/index.html#discussion",
    "title": "Classifying Palmer Penguins",
    "section": "Discussion",
    "text": "Discussion\nIn short, the strongest model I found for classifying penguins in the Palmer penguins dataset used a LogisticRegression with features of culmen length, culmen depth, and sex. While these features would have performed weakly on their own, when combined together, they formed a strong set of criteria for classification. The interplay between sex and culmen size was an interesting revelation brought about by the model, and exemplified the biggest strength of machine learning: finding unexpected patterns and using them to make highly accurate predictions. This example also exemplified the importance of simplicity in machine learning. Models like the GaussianProcessClassifier and RandomForestClassifier performed worse than the more simple LogisticRegression. When strong trends exist in datasets, we need not reinvent the wheel and overcomplicate the methods of classification. In order for the model to be generalizable to new data, it must make decisions based on logical trends, not just based on localized patterns within the training dataset."
  },
  {
    "objectID": "posts/blog-post-2-design/index.html",
    "href": "posts/blog-post-2-design/index.html",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "",
    "text": "Abstract\nWhen prospective borrowers request loans from banks, the bank is faced with a fiscal decision to provide the loan or reject the loan. Should the borrower default on the loan, the bank will likely take a loss. However, should they repay the loan, the bank will turn a profit through interest. This blog post will explore one methodology for an automated decision making process for granting or rejecting loans, using weights derived from a machine learning model. These weights will be used to determine the optimal threshold at which the bank should grant or deny loans, assuming we want to optimize for profit. The fairness of this model will then be examined, with a discussion of whether it is acceptable for certain people to have an easier time accessing credit over others.\n\n\nExploration\nWe will be working with a dataset of borrowers, with a target variable of whether they ended up defaulting on the loan or not. First, let’s take a look at the data:\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\n\n\ndf_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10\n\n\n\n\n\n\n\nIn this dataframe, loan_status is the target variable, which is a 0 if the loan was repaid, and a 1 if the borrower defaulted on the loan. We will therefore need to split this column from the rest of the data. We will also drop the loan_grade column, as that is the bank’s evaluation of how likely the borrower is to repay the loan (which is what we want to figure out ourselves!). Lastly, we will drop any columns with missing data, and convert any qualitative columns to one-hot columns so that our eventual machine learning model will have an easier time with the features.\n\n# Pre-processing\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(df_train[\"loan_status\"]) # for 0, 1, 2 etc. cols\n\ndef prepare_data(df):\n  df = df.dropna()\n  y = le.transform(df[\"loan_status\"])\n  df = df.drop([\"loan_status\"], axis = 1)\n  df = df.drop([\"loan_grade\"], axis = 1)\n  df = pd.get_dummies(df, dtype=int)\n  return df, y\n\nX_train, y_train = prepare_data(df_train)\nX_train\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\nperson_home_ownership_MORTGAGE\nperson_home_ownership_OTHER\nperson_home_ownership_OWN\nperson_home_ownership_RENT\nloan_intent_DEBTCONSOLIDATION\nloan_intent_EDUCATION\nloan_intent_HOMEIMPROVEMENT\nloan_intent_MEDICAL\nloan_intent_PERSONAL\nloan_intent_VENTURE\ncb_person_default_on_file_N\ncb_person_default_on_file_Y\n\n\n\n\n1\n27\n98000\n3.0\n11750\n13.47\n0.12\n6\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n1\n\n\n2\n22\n36996\n5.0\n10000\n7.51\n0.27\n4\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n1\n0\n\n\n3\n24\n26000\n2.0\n1325\n12.87\n0.05\n4\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n1\n0\n\n\n4\n29\n53004\n2.0\n15000\n9.63\n0.28\n10\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n\n\n6\n21\n21700\n2.0\n5500\n14.91\n0.25\n2\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n1\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n26059\n36\n150000\n8.0\n3000\n7.29\n0.02\n17\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n\n\n26060\n23\n48000\n1.0\n4325\n5.42\n0.09\n4\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n1\n0\n\n\n26061\n22\n60000\n0.0\n15000\n11.71\n0.25\n4\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n1\n0\n\n\n26062\n30\n144000\n12.0\n35000\n12.68\n0.24\n8\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n0\n\n\n26063\n25\n60000\n5.0\n21450\n7.29\n0.36\n4\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n1\n0\n\n\n\n\n22907 rows × 19 columns\n\n\n\nNow we are ready to begin exploring the data. It will be useful to have some idea of potential correlations between features before training the model, as this will help us understand the results we obtain. One potential correlation I thought would be interesting to look at is what interest rate the bank offers a borrower as it relates to the length of the borrower’s last employment. Let’s first take a look at the minimum, maximum, and average of these features.\n\nprint(\"min interest rate: \", X_train[\"loan_int_rate\"].min())\nprint(\"max interest rate: \", X_train[\"loan_int_rate\"].max())\nprint(\"mean interest rate: \", X_train[\"loan_int_rate\"].mean())\n\nprint(\"\\n\")\nprint(\"min emp length: \", X_train[\"person_emp_length\"].min())\nprint(\"max emp length: \", X_train[\"person_emp_length\"].max())\nprint(\"mean emp length: \", X_train[\"person_emp_length\"].mean())\n\nmin interest rate:  5.42\nmax interest rate:  23.22\nmean interest rate:  11.03494608634915\n\n\nmin emp length:  0.0\nmax emp length:  123.0\nmean emp length:  4.787357576286724\n\n\nThe minimum interest rate that the bank offered a borrower was 5.42%, the maximum was 23.22%, and the mean falls at around 11%. The minimum last employment length was zero, which makes sense, as some borrowers have likely never worked before. The maximum was 123 years, which is almost certainly an error in the data, especially considering that the average was about 4.75 years. For the sake of simplicity, we will consider last employment lengths from 0 to 20 years, and use a chart to visualize what proportion of different employment length buckets received low interest rates versus high interest rates. The following code block generates Figure 1, which shows this relationship.\n\n# Visualization 1\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nX_train_dv = X_train.copy()\n\n# Define employment length bins\nemp_bins = list(range(0, 24, 3))  # Bins: [0-2, 3-5, 6-8, ..., 18-20]\nemp_labels = [\"0-2\", \"3-5\", \"6-8\", \"9-11\", \"12-14\", \"15-17\", \"18-20\"]\n\n# Categorize employment length into bins\nX_train_dv[\"emp_length_group\"] = pd.cut(X_train_dv[\"person_emp_length\"], bins=emp_bins, labels=emp_labels, right=True, include_lowest=True)\n\n# Categorize interest rates\nX_train_dv[\"rate_category\"] = X_train_dv[\"loan_int_rate\"].apply(lambda x: \"Below 11%\" if x &lt; 11 else \"Above 11%\")\n\n# Create the bar plot\nplt.figure(figsize=(12, 6))\nsns.countplot(\n    data=X_train_dv, \n    x=\"emp_length_group\", \n    hue=\"rate_category\",\n    palette=\"Set1\"\n)\n\n# Customize labels\nplt.xlabel(\"Employment Length (Years)\")\nplt.ylabel(\"Count of Individuals\")\nplt.title(\"Fig 1: Interest Rate Distribution by Employment Length\")\nplt.legend(title=\"Interest Rate Category\")\nplt.xticks(rotation=45)\n\nplt.show()\n\n\n\n\n\n\n\n\nAs we can see, borrowers with last employment lengths of 0-2 years receive above average interest rates more often than borrowers with longer last employment lengths. This suggests that borrowers who have had strong careers are offered more favorable deals than borrowers who have not worked stable for lengthy periods of time. The bank may do this as a form of risk protection, as a higher interest rate will encourage a borrower to repay their loan in a timely manner. However, this may have disparate impacts on people whose last employment length was low, as not having had a long-term job suggests that they may lack savings. Taking on a high-interest loan may be unviable for them, so this distribution may impact some borrowers’ ability to access affordable credit.\n\n# Visualization 2\nplt.figure(figsize=(10, 5))\nsns.countplot(\n    data=df_train, \n    x=\"person_home_ownership\", \n    hue=\"loan_intent\",\n    palette=\"Set2\"\n)\n\nplt.xlabel(\"Home Ownership Status\")\nplt.ylabel(\"Count\")\nplt.title(\"Fig 2: Loan Intent by Home Ownership Status\")\nplt.legend(title=\"Loan Intent\")\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2 plots the relation between loan intent and home ownership status. Renters have the highest rates of education and medical loans, mortgagers have a high rate of education loans as well as personal and debt consolidation loans. Home owners, on the other hand, have disproportionately high rates of venture loans. Venture loans, importantly, are the type of loan most likely to imply that its use is to expand the borrower’s capital even further. Medical loans or debt consolidation loans may be necessary for the borrower’s continued well-being, whereas venture loans are likely optional loans being taken because there is an opportunity for fiscal gain. What this suggests is that home renters or mortgagers may be in a less stable position than homeowners, which means that their loans are more focused on continued survival rather than development of wealth. If it turns out that venture loans are more likely to be repaid than medical loans, for example, this may lead a profit-driven bank to provide fewer essential loans and more investment loans, which can have strong negative impacts on those in need.\n\n# Summary Table\nsummary_table = df_train.groupby(\"cb_person_default_on_file\").aggregate(\n    Mean_Income_USD=(\"person_income\", \"mean\"),\n    Median_Income_USD=(\"person_income\", \"median\"),\n    Mean_Loan_Size_USD=(\"loan_amnt\", \"mean\"),\n    Median_Loan_Size_USD=(\"loan_amnt\", \"median\")\n)\n\n# Display the table\nprint(summary_table)\n\n                           Mean_Income_USD  Median_Income_USD  \\\ncb_person_default_on_file                                       \nN                             65990.726858            55315.0   \nY                             65900.076251            54000.0   \n\n                           Mean_Loan_Size_USD  Median_Loan_Size_USD  \ncb_person_default_on_file                                            \nN                                 9464.480849                8000.0  \nY                                10062.338868                8100.0  \n\n\nThe summary table above displays the mean and median income as well as mean and median loan size, organized by whether the borrowers had a default on file or not. My suspicion before making this table was that borrowers with a default on file may have lower income than borrowers without one, and that borrowers with a default on file may only be approved for smaller loans than borrowers without one. Interestingly, neither of my predicted relationships seem to hold, as mean and median income are very similar across both categories, and mean loan size is actually higher for those with a default on file than those without one. This may suggest that having a default on file is not a significant factor in the bank’s decision making, and that having a default on file is not correlated with income. Therefore, our machine learning model may place relatively little weight on loan size or income.\n\n\nModeling\nNow it is time to train a model to predict whether a borrower will default or not. This will result in a vector of weights \\(\\mathbf{w}\\), which we can then use to decide whether loans should be approved or not.\nTo begin, we’ll need to choose what features we want to use to train the model– for simplicity, we can simply provide the model with all of the features as a starting point, and evaluate its accuracy. In many cases, this approach is effective out of the box, and won’t require any further selection of features.\nThe following code block simply trains a LogisticRegression on all columns of X_train (except loan_grade of course, since it was dropped earlier). The only further transformation we will apply to the data is a logorithmic scale to income, as the wide range of values present in the income column is cumbersome for the LogisticRegression. By scaling it, the LR can converge more efficiently. We will then use 5-fold cross validation to simulate a test of the model, and evaluate the average accuracy. The output will contain some warnings regarding the number of iterations, due to the scale of the data being very large– we could increase the number of iterations to account for this, but this process is a good starting point, and leaving the number of iterations at default maintains a low running time.\n\n# Train an LR using all features except loan grade\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\n\nX_train[\"person_income\"] = np.log1p(X_train[\"person_income\"])  # log scale person income due to wide range\n\nLR = LogisticRegression()\nLR.fit(X_train, y_train)\nscores = cross_val_score(LR, X_train, y_train, cv=5) # Using 5-fold cross validation\n\n/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n\nprint(np.mean(scores))\n\n0.8159947464862315\n\n\nThe model achieved an average accuracy of 81.6%, which is reasonably accurate for this sort of prediction. Embedded in this model is our vector of weights \\(\\mathbf{w}\\), which we can use to compute a borrower score for each borrower in the training set. The LR uses this score to make a prediction on its own, but we will manually use the vector to make our own prediction by determing the cutoff point at which we can maximize profit. For simplicity, I will use the provided mathematical model, where the profits for the case where the loan is repaid and the case where the loan is defaulted are as follows:\n\\[\n\\text{Repaid Profit} = \\text{Loan Amount} \\times (1 + 0.25 \\times \\text{Loan Interest Rate})^{10} - \\text{Loan Amount}\n\\]\n\\[\n\\text{Defaulted 'Profit'} = \\text{Loan Amount} \\times (1 + 0.25 \\times \\text{Loan Interest Rate})^{3} - 1.7 \\times \\text{Loan Amount}\n\\]\nThe following code block uses this mathematical model to determine the optimal threshold at which we should stop accepting loans. It first calculates the score for each borrower, then creates a new dataframe which contains the borrower’s score, the outcome of their loan (whether they repaid it or defaulted on it), and the profit made from their loan in either case as determined by the mathematical model above. It then adds a new column to the dataframe, expected profit, which contains the actual profit made if this loan is accepted based on the outcome of the loan and the profit calculation. Lastly, it calculates the cumulative profit made if we accept all borrowers above every possible threshold, then prints out the threshold at which the maximum cumulative profit is made, as well as what that profit is, and what the profit per borrower is.\n\n# Find the optimal profit threshold based on the predictions made by the LR\nborrower_scores = X_train.values @ LR.coef_.ravel() # ravel method fixes dimensions of coef vector\nborrower_scores = borrower_scores.flatten() # flatten method converts ~20000 * 1 matrix to a 1d array of length ~20000\n\nrepay_profit = X_train[\"loan_amnt\"] * (1 + 0.25*(X_train[\"loan_int_rate\"]/100))**10 - X_train[\"loan_amnt\"]\ndefault_profit = X_train[\"loan_amnt\"] * (1 + 0.25 *(X_train[\"loan_int_rate\"]/100))**3 - 1.7*X_train[\"loan_amnt\"]\n\ndf_scores = pd.DataFrame({\n    \"score\": borrower_scores,\n    \"outcome\": y_train,\n    \"repay_profit\": repay_profit,\n    \"default_profit\": default_profit\n})\n\ndf_scores = df_scores.sort_values(by=\"score\", ascending=False).reset_index(drop=True) # sort so we can test thresholds from top to bottom\n\ndf_scores[\"expected_profit\"] = df_scores[\"outcome\"] * df_scores[\"repay_profit\"] + (1 - df_scores[\"outcome\"]) * df_scores[\"default_profit\"]\ndf_scores = df_scores.sort_values(by=\"score\")\nsorted_scores = df_scores[\"score\"].values\nsorted_profits = df_scores[\"expected_profit\"].values\n\n# Compute cumulative profit in reverse order\ncumulative_profit = np.cumsum(sorted_profits[::-1])[::-1] # cumulative profit across all approved borrowers\nnum_borrowers = np.arange(len(sorted_scores), 0, -1) # total num of borrowers\nprofit_per_borrower = cumulative_profit / num_borrowers  \n\n# Find the threshold that maximizes profit\noptimal_index = np.argmax(cumulative_profit)\noptimal_threshold = sorted_scores[optimal_index]\noptimal_profit = cumulative_profit[optimal_index]\noptimal_PPB = profit_per_borrower[optimal_index]\n\nprint(\"Optimal Threshold: \", optimal_threshold)\nprint(\"Optimal Profit: \", optimal_profit)\nprint(\"Optimal PPB: \", optimal_PPB)\n\nOptimal Threshold:  0.14241452471915317\nOptimal Profit:  3313401.632051698\nOptimal PPB:  2754.2823209074795\n\n\nThis model imputes an optimal score threshold of about 0.14, at which the profit on the training set is calculated to be 3.3 million dollars, with a profit per borrower of $2754.28. The following code block visualizes all possible thresholds as a line, showing clearly where the maxima lies.\n\nplt.figure(figsize=(10, 5))\nplt.plot(sorted_scores, cumulative_profit, label=\"Cumulative Profit\")\nlabel1 = (\"Optimal Threshold: \" + str(float(optimal_threshold)))\nplt.axvline(optimal_threshold, color='r', linestyle='dashed', label=label1)\nlabel2 = (\"Optimal Profit: $\" + str(round(float(optimal_profit), 2)))\nplt.scatter(optimal_threshold, optimal_profit, color='green', zorder=3, s=17, label = label2)\n\nplt.xlabel(\"Borrower Score (Threshold)\")\nplt.xlim(0.0, 0.5)\nplt.ylabel(\"Cumulative Profit (millions USD)\")\nplt.ylim(2000000, 3500000)\nplt.title(\"Optimized Profit vs. Score Threshold\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nEvaluation\nTo properly evaluate this model, we must now test it out on a test data set to ensure that it is not subject to overfitting on the training set. To accomplish this, we will pull the test data and apply the same preparation steps that we took with the training data. We will then apply our calculated optimal threshold, and determine the profit if we accept all borrowers in the test set with scores above the threshold.\n\n# Evaluate on test dataset\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\n\nX_test, y_test = prepare_data(df_test)\nX_test[\"person_income\"] = np.log1p(X_test[\"person_income\"])  # log scale person income due to wide range\n\n\nborrower_scores_test = X_test.values @ LR.coef_.ravel()\nborrower_scores_test = borrower_scores_test.flatten()\n\nthreshold = 0.14241452471915317\nloan_approved = borrower_scores_test &gt;= threshold\nrepay_profit_test = X_test[\"loan_amnt\"] * (1 + 0.25*(X_test[\"loan_int_rate\"]/100))**10 - X_test[\"loan_amnt\"]\ndefault_profit_test = X_test[\"loan_amnt\"] * (1 + 0.25 *(X_test[\"loan_int_rate\"]/100))**3 - 1.7*X_test[\"loan_amnt\"]\n\ndf_scores_test = pd.DataFrame({\n    \"score\": borrower_scores_test,\n    \"outcome\": y_test,\n    \"repay_profit\": repay_profit_test,\n    \"default_profit\": default_profit_test\n})\n\ndf_scores_test[\"expected_profit\"] = df_scores_test[\"outcome\"] * df_scores_test[\"repay_profit\"] + (1 - df_scores_test[\"outcome\"]) * df_scores_test[\"default_profit\"]\n\nprofit_test = np.where(\n    loan_approved,\n    df_scores_test[\"expected_profit\"],\n    0  # 0 change if the loan is denied\n)\n\nrevenue = profit_test[profit_test &gt; 0]\ntotal_profit_test = revenue.sum()\nprofit_per_borrower_test = revenue.mean()\n\nprint(\"Total Profit: \", total_profit_test)\nprint(\"Average PPB: \", profit_per_borrower_test)\nprint(\"Approval Rate:\", (loan_approved.sum() / len(df_test)) * 100, \"%\")\nprint(\"Approved Loans: \", len(revenue))\nprint(\"Total Requested Loans: \", len(profit_test))\n\nTotal Profit:  365142.9799703294\nAverage PPB:  8693.880475484033\nApproval Rate: 0.7365352155899954 %\nApproved Loans:  42\nTotal Requested Loans:  5731\n\n\nAs we can see, the model is profitable on the test set as well, with a profit of $365,142.98, and a profit per borrower of $8693.88. These scores indicate a high level of profitability for the bank, and show that the model generalizes effectively. However, we also see that the approval rate is very low, at 0.74%. Out of 5731 prospective borrowers, only 42 were given loans under this pure capital optimization model. While this may be perfect for the bank in terms of making money, the borrower adds another side to the story.\n\n\nThe Borrower’s Perspective\n\nIs it more difficult for people in certain age groups to access credit under your system?\n\nTo answer this question, let’s take a look at the rows in X_test (the borrowers) that were approved to get a loan based on the optimal threshold I determined. The following line of code prints the ages of each approved borrower. The line after prints the min, max, and average age of the approved borrowers.\n\nprint(X_test[loan_approved &gt; 0][\"person_age\"])\n\n241     23\n276     31\n312     22\n318     23\n339     31\n350     25\n391     23\n437     24\n489     29\n550     22\n650     23\n705     21\n952     27\n1050    30\n1211    21\n1241    27\n1311    22\n1423    27\n1669    25\n2086    23\n2218    23\n2358    22\n2375    22\n2434    24\n3492    28\n3608    23\n3763    21\n3821    21\n3974    23\n4098    23\n4116    24\n4243    22\n4542    22\n4740    28\n4785    21\n4901    23\n4945    27\n4980    23\n5316    22\n5592    22\n5645    27\n5924    28\n6013    27\n6015    26\n6102    21\n6118    24\n6447    22\n6491    27\nName: person_age, dtype: int64\n\n\n\nprint(X_test[loan_approved &gt; 0][\"person_age\"].min())\nprint(X_test[loan_approved &gt; 0][\"person_age\"].max())\nprint(X_test[loan_approved &gt; 0][\"person_age\"].mean())\n\n21\n31\n24.270833333333332\n\n\nAs we can see, the youngest borrower who was approved for a loan was 21 years old, and the oldest one was 31 years old. This is a tight one decade range, which suggests that older borrowers are being unilaterally denied. To see how significant the impact is, let’s take a look at how many prospective borrowers there were that are over the age of 31.\n\nprint((X_test[\"person_age\"] &gt; 31).sum())\n\n1180\n\n\n1180 borrowers above the age of 31 were denied credit. While it is possible (and likely) that factors other than age played a role in determining whether these borrowers would be approved or not, this result still suggests that the model unfairly trends towards younger borrowers.\n\nIs it more difficult for people to get loans in order to pay for medical expenses? How does this compare with the actual rate of default in that group? What about people seeking loans for business ventures or education?\n\nTo answer this question, let’s take a look at the proportion of approval and default for medical loans, education loans, and venture loans. The following code block computes the approval rate and default rate in each category of loan, then displays the results in a summary table.\n\ncategories = [\"loan_intent_MEDICAL\", \"loan_intent_EDUCATION\", \"loan_intent_VENTURE\"]\nloan_approved_dv = df_scores_test[\"score\"] &gt;= optimal_threshold\nsummary_data = []\n\n# Loop through each category and compute metrics\nfor category in categories:\n    # Filter for loans in the current category\n    category_mask = X_test[category] == 1\n\n    # Calculate approval rate and default rate\n    approval_rate = loan_approved_dv.loc[category_mask].mean() * 100  # % of loans approved in current category\n    default_rate = (y_test[category_mask] == 1).mean() * 100  # % of loans that defaulted in current category\n\n    # Append results to summary data\n    summary_data.append([category.replace(\"loan_intent_\", \"\").title(), approval_rate, default_rate])\n\n# Create DataFrame\nsummary_table = pd.DataFrame(summary_data, columns=[\"Loan Intent\", \"Approval Rate (%)\", \"Default Rate (%)\"])\n\n# Display summary table\nprint(summary_table)\n\n  Loan Intent  Approval Rate (%)  Default Rate (%)\n0     Medical           1.118360         28.424977\n1   Education           0.850340         16.751701\n2     Venture           0.414938         14.626556\n\n\nBased on this summary table, borrowers with medical intent have the highest rate of default, at 28%. In spite of this, they have a high approval rate in my model at 1.11%. Education and venture loans have lower rates of default, but lower rates of approval as well. What this suggests is that the LogisticRegression model we trained earlier likely did not consider loan intent as a highly weighted category for prediction, as, if it did, it is likely that fewer medical loans would get approved than venture loans.\n\nHow does a person’s income level impact the ease with which they can access credit under your decision system?\n\nThe following code block prints the minimum, maximum, mean, and median income of borrowers who were approved, then borrowers who were not approved. Because we applied a logarithmic scale to the income column in the process of data preparation, we will go back to the original test dataframe to perform this, with the only transformation being dropping the same rows we dropped in the preparation so that the arrays are the same length.\n\ndf_test = df_test.dropna()\nprint(df_test[loan_approved &gt; 0][\"person_income\"].min())\nprint(df_test[loan_approved &gt; 0][\"person_income\"].max())\nprint(df_test[loan_approved &gt; 0][\"person_income\"].mean())\nprint(df_test[loan_approved &gt; 0][\"person_income\"].median())\n\nprint(\"\\n\")\nprint(df_test[loan_approved == 0][\"person_income\"].min())\nprint(df_test[loan_approved == 0][\"person_income\"].max())\nprint(df_test[loan_approved == 0][\"person_income\"].mean())\nprint(df_test[loan_approved == 0][\"person_income\"].median())\n\n21000\n168000\n58263.395833333336\n52000.0\n\n\n4800\n1782000\n66670.4568009854\n55000.0\n\n\nThe lowest income of an accepted borrower was $21,000 USD, and the maximum was $168,000 USD. The average was $58,263.40 USD, and the median was $52,000 USD. For rejected borrowers, the minimum income was $4,800 USD, the maximum was 1.7 million USD, the average was $66,670.46 USD, and the median was $55,000 USD. What this suggests is that very low income borrowers are unable to access credit, as anyone with under $21,000 USD in yearly income is denied credit. That being said, $21,000 USD per year is achievable with a minimum wage job, so this suggests that the bar for being of enough income to be approved is having a job. This being said, the median incomes suggest that the mean for the denied borrowers may be unusually high due to outliers. The medians are very similar, which suggests that income is not a major factor in accessing credit under this model.\n\n\nDiscussion\nIn this modeling exercise, we found that optimizing profit often leads to very low lending rates for banks. The optimal approval rate for the bank was only 0.73%, which shows that making the most possible money leads to very limited access to credit for borrowers. Interestingly, the model did not discriminate based on loan intent much at all, despite strongly differing rates of default between different intention categories. Age was a seemingly important factor, with no prospective borrower above the age of 31 being granted a loan. It could be argued that using all features to train the LogisticRegression model introduced opportunities for bias in the decisions, as if people 31 years or older, for example, show a high rate of default, they may have more difficulty accessing credit under this automated decision systems. In order for systems to be fair, it is possible that optimizing purely for profit is not ideal, as patterns in data may produce uneven outcomes for borrowers, which is ideally to be avoided.\n\nConsidering that people seeking loans for medical expenses have a high rate of default, is it fair that it is more difficult for them to obtain access to credit?\n\nIn answering this question, I apply the following definition of fairness: In a fair system, all parties have equal chances of succeeding at, or taking advantage of an opportunity.\nUnder this definition, I find that it is technically fair for people seeking loans for medical expenses to have a harder time obtaining access to credit. As long as this increased difficulty is equally true for all people seeking medical loans, and they are subject to the same decision making process, it is technically fair. However, the outcomes could still be unfair even if the decision making process is fair. For example, if part of the decision for whether to give a medical loan is examining the client’s income, this could give some clients an unfair advantage over others depending on their access to high-paying jobs (which depends on their access to education, which depends hugely on where they were born, which they had no control over). I believe that in an ideal, fair world, everyone seeking medical care would have equal access to it, but that is not the world we live in. Given the unfair world we live in, I find that a decision making process that makes it more difficult for people seeking medical loans to access credit is fair."
  }
]