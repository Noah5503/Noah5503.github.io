[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Noah P",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Implementing Logistic Regression\n\n\n\n\n\nNoah Price Blog Post 7 - CSCI0451\n\n\n\n\n\nApr 28, 2025\n\n\nNoah Price\n\n\n\n\n\n\n\n\n\n\n\n\nOverfitting, Overparameterization, and Double Descent\n\n\n\n\n\nNoah Price Blog Post 6 - CSCI0451\n\n\n\n\n\nApr 21, 2025\n\n\nNoah Price\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing the Perceptron Algorithm\n\n\n\n\n\nNoah Price Blog Post 5 - CSCI0451\n\n\n\n\n\nMar 31, 2025\n\n\nNoah Price\n\n\n\n\n\n\n\n\n\n\n\n\nLimits of the Quantitative Approach to Fairness\n\n\n\n\n\nNoah Price Blog Post 4 - CSCI0451\n\n\n\n\n\nMar 26, 2025\n\n\nNoah Price\n\n\n\n\n\n\n\n\n\n\n\n\nAuditing Bias\n\n\n\n\n\nNoah Price Blog Post 3 - CSCI0451\n\n\n\n\n\nMar 12, 2025\n\n\nNoah Price\n\n\n\n\n\n\n\n\n\n\n\n\nDesign and Impact of Automated Decision Systems\n\n\n\n\n\nNoah Price Blog Post 2 - CSCI0451\n\n\n\n\n\nMar 5, 2025\n\n\nNoah Price\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\nNoah Price Blog Post 1 - CSCI0451\n\n\n\n\n\nFeb 21, 2025\n\n\nNoah Price\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/blog-post-1-penguins/index.html",
    "href": "posts/blog-post-1-penguins/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "The Palmer Penguins dataset provides a set of physical characteristics of penguins of three different species: Adélie, Chinstrap, and Gentoo. Using all the physical characteristics to train a model would invariably yield highly accurate results, however, there may exist strong enough trends in the dataset to support a highly accurate model using a smaller number of features. This analysis will cover an examination of the Palmer Penguins dataset, as well as a generalizable method for determining the three strongest features for classifying the penguins. Discussion will go into strengths and weaknesses of various models and features, as well as justifying why the strongest performing model was best suited to the task at hand."
  },
  {
    "objectID": "posts/blog-post-1-penguins/index.html#abstract",
    "href": "posts/blog-post-1-penguins/index.html#abstract",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "The Palmer Penguins dataset provides a set of physical characteristics of penguins of three different species: Adélie, Chinstrap, and Gentoo. Using all the physical characteristics to train a model would invariably yield highly accurate results, however, there may exist strong enough trends in the dataset to support a highly accurate model using a smaller number of features. This analysis will cover an examination of the Palmer Penguins dataset, as well as a generalizable method for determining the three strongest features for classifying the penguins. Discussion will go into strengths and weaknesses of various models and features, as well as justifying why the strongest performing model was best suited to the task at hand."
  },
  {
    "objectID": "posts/blog-post-1-penguins/index.html#exploration",
    "href": "posts/blog-post-1-penguins/index.html#exploration",
    "title": "Classifying Palmer Penguins",
    "section": "Exploration",
    "text": "Exploration\nFirst, let’s access the training dataset and take a look at it.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\nIn order for our models to train effectively on these features, we’ll want to transform the columns to make them more machine-friendly. In particular, we will separate the species column from the rest of the data and encode species as integers (0 for Adélie, 1 for Chinstrap, and 2 for Gentoo). This will allow us to train on the data without the species labels, then check our work against the labels. We will also drop any rows with missing data so we only work with penguins for which we have complete data. Lastly, we will encode qualitative features such as the island the penguin was found on or the penguin’s sex as “one-hot columns”, meaning they will either be 1 for true or 0 for false.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df, dtype=int)\n  return df, y\n\nX_train, y_train = prepare_data(train)\nX_train\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\n0\n1\n0\n1\n0\n1\n1\n0\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\n0\n1\n0\n1\n0\n1\n0\n1\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\n1\n0\n0\n1\n0\n1\n0\n1\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\n1\n0\n0\n1\n0\n1\n1\n0\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\n0\n1\n0\n1\n0\n1\n0\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n270\n51.1\n16.5\n225.0\n5250.0\n8.20660\n-26.36863\n1\n0\n0\n1\n0\n1\n0\n1\n\n\n271\n35.9\n16.6\n190.0\n3050.0\n8.47781\n-26.07821\n0\n0\n1\n1\n1\n0\n1\n0\n\n\n272\n39.5\n17.8\n188.0\n3300.0\n9.66523\n-25.06020\n0\n1\n0\n1\n0\n1\n1\n0\n\n\n273\n36.7\n19.3\n193.0\n3450.0\n8.76651\n-25.32426\n0\n0\n1\n1\n0\n1\n1\n0\n\n\n274\n42.4\n17.3\n181.0\n3600.0\n9.35138\n-24.68790\n0\n1\n0\n1\n0\n1\n1\n0\n\n\n\n\n256 rows × 14 columns\n\n\n\nBefore getting into using models for classification, it will be helpful to examine some of the features to get some preliminary ideas of what good choices for features will be. For the purposes of visualization only, we will create a dataframe containing both the features and the species of the penguins, so that we can plot them against each other. The following code block will create this dataframe, and use it to generate our first data visualization.\n\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt \n\nspecies_mapping = {0: \"Adélie\", 1: \"Chinstrap\", 2: \"Gentoo\"}\nspecies = np.vectorize(species_mapping.get)(y_train) # Create array with species names\n\n# Create a copy of X_train which has the species names (used only for visualizations!)\nX_train_dv = X_train.copy()\nX_train_dv[\"Species\"] = species\n\nsns.scatterplot(\n    x=X_train_dv[\"Flipper Length (mm)\"],\n    y=X_train_dv[\"Body Mass (g)\"],\n    hue=X_train_dv[\"Species\"],\n    palette=\"Set1\"\n)\n\nplt.xlabel(\"Flipper Length (mm)\")\nplt.ylabel(\"Body Mass (g)\")\nplt.title(\"Fig. 1: Penguin Flipper Length and Body Mass by Species\")\nplt.legend(title=\"Species\")\nplt.show()\n\n\n\n\n\n\n\n\nFig. 1 plots the penguins based on their body mass on the y-axis, and flipper length on the x-axis. These qualitative features seemed like a good starting point, as, together, they provide a reasonable estimate of a penguin’s overall size and shape, which may be indicative of species. What’s notable about this figure is that there appear to be two ‘clusters’: one on the lower end of the chart with a fair mix of Adélie and Chinstrap penguins, and one on the higher end with almost all Gentoo penguins. What this suggests is that these quantitative features may not be sufficient on their own to distinguish between species, as Adélie and Chinstrap penguins sit in a cluster; however, these features would be highly effective for identifying Gentoo penguins, since they have a tendency to be larger than the other types.\n\nsns.scatterplot(\n    x=X_train_dv[\"Delta 15 N (o/oo)\"],\n    y=X_train_dv[\"Delta 13 C (o/oo)\"],\n    hue=X_train_dv[\"Species\"],\n    palette=\"Set1\"\n)\n\nplt.xlabel(\"Delta 15 N (o/oo)\")\nplt.ylabel(\"Delta 13 C (o/oo)\")\nplt.title(\"Fig. 2: Delta 13 C and Delta 15 N by Species\")\nplt.legend(title=\"Species\")\nplt.show()\n\n\n\n\n\n\n\n\nFig. 2 plots the penguins by species based on their Delta 13 C and Delta 15 N, which are measurements of the carbon isotopes and nitrogen isotopes present in the penguin’s blood. The ratios of these isotopes is related to the penguin’s diet, meaning it could be different between different species if they eat different types of food. The figure suggests a loose correlation within each group– the Gentoo penguins trend towards the bottom left, the Chinstrap penguins trend towards the top right, and the Adélie penguins are somewhere in the middle. However, it is notable that Adélie penguins also appear in the Chinstrap and Gentoo clusters, meaning this quantitative feature may be effective for identifying Chinstrap and Gentoo penguins, but poor for identifying Adélie penguins.\n\n# Compute mean and median for culmen length and depth\nsummary_table = X_train_dv.groupby(\"Species\").aggregate(\n    Mean_Culmen_Length_mm=(\"Culmen Length (mm)\", \"mean\"),\n    Median_Culmen_Length_mm=(\"Culmen Length (mm)\", \"median\"),\n    Mean_Culmen_Depth_mm=(\"Culmen Depth (mm)\", \"mean\"),\n    Median_Culmen_Depth_mm=(\"Culmen Depth (mm)\", \"median\")\n)\n\n# Display the table\nprint(summary_table)\n\n           Mean_Culmen_Length_mm  Median_Culmen_Length_mm  \\\nSpecies                                                     \nAdélie                 38.961111                    38.90   \nChinstrap              48.771429                    49.25   \nGentoo                 47.133696                    46.55   \n\n           Mean_Culmen_Depth_mm  Median_Culmen_Depth_mm  \nSpecies                                                  \nAdélie                18.380556                   18.50  \nChinstrap             18.346429                   18.25  \nGentoo                14.926087                   14.80  \n\n\nThis summary table shows the mean and median values of culmen length and culmen depth by species. Adélie penguins have low average culmen lengths, and Gentoo penguins have low average culmen depths. Chinstrap penguins have high scores on both. If only one of these features were used, it would be difficult to classify all three types, as two of the species are similar on each feature individually. However, when looking at both features at the same time, the pairs of scores seem to be strong potential indicators of species. Adélie penguins should have low length but high depth, Gentoos should have high length and low depth, and Chinstraps should have both. To finish our data examination, let’s now move to some qualitative features.\n\nisland_columns = [\"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"]\nX_train_dv[\"island\"] = X_train_dv[island_columns].idxmax(axis=1).str.replace(\"Island_\", \"\")\n\n# Count the number of penguins by island and species\nisland_species_counts = X_train_dv.groupby([\"island\", \"Species\"]).size().reset_index(name=\"count\")\n\n# Plot using seaborn barplot\nplt.figure(figsize=(8, 5))\nsns.barplot(x=\"island\", y=\"count\", hue=\"Species\", data=island_species_counts, palette=\"Set2\")\n\nplt.xlabel(\"Island\")\nplt.ylabel(\"Number of Penguins\")\nplt.title(\"Fig. 3: Penguin Species Counts by Island\")\nplt.legend(title=\"Species\")\nplt.show()\n\n\n\n\n\n\n\n\nFig. 3 displays the species counts by island. This chart holds some initially promising results– Gentoo penguins are only found on Biscoe Island, and Chinstrap penguins are only found on Dream island. However, there are also some problems, in that Adélie penguins are found on all three islands (though at different proportions), and, more subtly, this may suggests that the model could overfit based on these features. If, for example, the testing dataset contains a Gentoo or Chinstrap penguin found on Torgersen island, a model trained on this feature would likely predict that penguin to be Adélie (unless other features strongly suggested otherwise). Unless this subset of the dataset correctly shows that there are no Gentoo or Chinstrap penguins on Torgersen island, this pattern has the potential to yield inaccurate predictions.\n\nsex_columns = [\"Sex_FEMALE\", \"Sex_MALE\"]\nX_train_dv[\"sex\"] = X_train_dv[sex_columns].idxmax(axis=1).str.replace(\"Sex_\", \"\")\n\n# Count the number of penguins by island and species\nsex_counts = X_train_dv.groupby([\"sex\", \"Species\"]).size().reset_index(name=\"count\")\n\n# Plot using seaborn barplot\nplt.figure(figsize=(8, 5))\nsns.barplot(x=\"Species\", y=\"count\", hue=\"sex\", data=sex_counts, palette=\"Set2\")\n\nplt.xlabel(\"Species\")\nplt.ylabel(\"Number of Penguins\")\nplt.title(\"Fig. 4: Penguin Species Counts by Sex\")\nplt.legend(title=\"Sex\")\nplt.show()\n\n\n\n\n\n\n\n\nLastly, Fig. 4 shows the proportions of sex by species in the dataset. The expected rate of sex over a wide enough dataset may seem, intuitively, to be 50%, but factors such as differing behaviors between sexes may change the proportions depending on how different species operate. In this dataset, the counts within each species are relatively close, with Adélie penguins having more males than females, and the others having more females than males. Given the lack of significant variance within each species, it seems unlikely based on this visualization that sex would be a strong predictor of species."
  },
  {
    "objectID": "posts/blog-post-1-penguins/index.html#modeling",
    "href": "posts/blog-post-1-penguins/index.html#modeling",
    "title": "Classifying Palmer Penguins",
    "section": "Modeling",
    "text": "Modeling\nNow it’s time to select a model and a set of features. I experimented with several models, including the RandomForestClassifier, GaussianProcessClassifier, and Naive-Bayes classifier. All of these models performed decently, attaining maximum training accuracies in the 90+ percent range. However, what I found to be the most effective was the good old LogisticRegression. LogisticRegressions are highly effective when differences in data are linearly bound, while other models perform better on highly non-linear distinctions. Therefore, if our dataset shows linear distinctions between types, a LogisticRegression may outperform other models. Given our visualizations earlier, it was easy to see how, in some cases at least, lines could be drawn between clusters of data points of each species. This suggests that the LR model may be well suited to the task.\nIn terms of selecting features, the relatively modest size of the dataset allows a relatively modest search to get the job done. Simply iterating through every possible combination of two quantitative features and one qualitative feature (and keeping track of the best scoring features) will suffice in this case, though this is not a scalable method for larger datasets. The code block below implements this process, selecting two quantitative columns and one qualitative column in each iteration, then training a LogisticRegression using those features. The model is then evaluated using five-fold cross validation to ensure that it does not overfit to the training dataset. At the end, it will print what the best features were, as well as the training accuracy of the model.\nThe only other notable implementation here is the application of a scaler to the quantitative columns. LogisticRegressions work by finding minimum points of a function; by scaling quantitative data such that it follows a more normal distribution, this process can be carried out more efficiently and effectively. Therefore, as a final preprocessing step, we will apply a StandardScaler to the quantitative columns, which will help the LR converge in fewer iterations.\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\n\nqual_cols = [\"Clutch Completion\", \"Sex\", \"Island\", \"Stage_Adult, 1 Egg Stage\"]\nquant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\n\nscaler = StandardScaler() # Scaling quantitative values decreases the num. of iterations needed for LR\nX_train_scaled = X_train.copy()\nX_train_scaled[quant_cols] = scaler.fit_transform(X_train[quant_cols])\n\nbest_cols = []\nbest_score = 0\nfor qual in qual_cols: \n  qual_cols = [col for col in X_train_scaled.columns if qual in col ]\n  for pair in combinations(quant_cols, 2):\n    cols = qual_cols + list(pair) \n    LR = LogisticRegression()\n    LR.fit(X_train_scaled[cols], y_train)\n    scores = cross_val_score(LR, X_train_scaled[cols], y_train, cv=5) # Using 5-fold cross validation here\n    mean_score = np.mean(scores)\n    if (mean_score &gt; best_score):\n      best_cols = cols\n      best_score = mean_score\n\nprint(best_cols)\nprint(best_score)\n\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n0.9922322775263952\n\n\nThe best features for the model turned out to be culmen length and culmen depth for the quantitative columns, along with sex for the qualitative column. Culmen length and culmen depth make sense, as the summary table showed that the two, when combined, could function as a very strong identifier. Sex is an unexpected result for the qualitative column, as the visualizations suggested no strong correlation between sex and species. However, it is possible that sex scored the best because it avoided overfitting to the dataset. The use of cross-validation means that overfitting will lead to poor scores, so it is possible that the other qualitative columns showed inaccurate patterns in the training data. In this case, a feature like sex which has only weak correlations could perform better than ones with misleading patterns."
  },
  {
    "objectID": "posts/blog-post-1-penguins/index.html#evaluation",
    "href": "posts/blog-post-1-penguins/index.html#evaluation",
    "title": "Classifying Palmer Penguins",
    "section": "Evaluation",
    "text": "Evaluation\nThe iterative process yielded a training accuracy of 99.2%, which seems fairly solid. To properly evaluate it, though, we must compare it to the base rate. The following code block first uses np.bincount to count instances of each species in y_train, then computes the base rate by dividing the number of penguins in the most popular species by the total number of species. This represents the accuracy rate we would achieve if we always predicted that a penguin’s species is whatever the most popular species is.\n\ncounts = np.bincount(y_train)\nres = counts.max() / counts.sum()\nprint(res)\n\n0.421875\n\n\nThe base rate is 42.1%, so we have significantly outperformed the base rate. Now, let’s take the selected columns and evaluate the LR model on the test dataset, making sure to apply the same preprocessing steps to the test data.\n\ncols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Sex_FEMALE', 'Sex_MALE']\nLR = LogisticRegression()\nLR.fit(X_train_scaled[cols], y_train)\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\nX_test[quant_cols] = scaler.transform(X_test[quant_cols]) # Apply the same scaling to the test dataset!\n\nLR.score(X_test[cols], y_test)\n\n0.9852941176470589\n\n\nThe model achieves a 98.5% accuracy rate on the testing dataset, which suggests that it has generalized relatively effectively (meaning it has not overfitted to the training data set). The following code block plots the decision regions for the LR model.\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = \"Culmen Length (scaled)\", \n            ylabel  = \"Culmen Depth (scaled)\", \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\nplot_regions(LR, X_train_scaled[cols], y_train)\n\n\n\n\n\n\n\n\nThe decision regions of the LR appear, on visual inspection, to be highly accurate, with one misplaced point in the Female chart and one just on the line in the Male chart. These decision regions further elucidate why sex was chosen for the qualitative feature– it seems that males have generally larger culmens than females, which allows classifications made based on culmen length and depth to be even more accurate. To get a better idea of how the model performed on the test data, we will look at the confusion matrix. The code block below generates the confusion matrix by re-making the predictions, then comparing them to the actual test data.\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = LR.predict(X_test[cols])\nC = confusion_matrix(y_test, y_test_pred)\nC\n\narray([[31,  0,  0],\n       [ 0, 10,  1],\n       [ 0,  0, 26]])\n\n\nIn the confusion matrix above, the left column represents penguins that were classified as Adélie, the middle column represents penguins which were classified as Chinstrap, and the right column represents penguins that were classified as Gentoo. The first row were the Adélie penguins, the second row were the Chinstrap penguins, and the third row were the Gentoo penguins. What this means is that the only misclassification that the model made was classifying a single Chinstrap penguin as a Gentoo penguin. Otherwise, 31 Adélie penguins, 10 Chinstrap penguins, and 26 Gentoo penguins were correctly identified. Thinking back to the features, it makes sense that the error the model made was classifying a Chinstrap penguin as a Gentoo penguin, as the distinction between Gentoos and the other types in terms of culmen depth was a smaller distinction than the difference between, e.g. Adélie penguins and the others in terms of culmen length. If a Chinstrap penguin had a particularly low culmen depth, the model could have classified it as a Gentoo penguin."
  },
  {
    "objectID": "posts/blog-post-1-penguins/index.html#discussion",
    "href": "posts/blog-post-1-penguins/index.html#discussion",
    "title": "Classifying Palmer Penguins",
    "section": "Discussion",
    "text": "Discussion\nIn short, the strongest model I found for classifying penguins in the Palmer penguins dataset used a LogisticRegression with features of culmen length, culmen depth, and sex. While these features would have performed weakly on their own, when combined together, they formed a strong set of criteria for classification. The interplay between sex and culmen size was an interesting revelation brought about by the model, and exemplified the biggest strength of machine learning: finding unexpected patterns and using them to make highly accurate predictions. This example also exemplified the importance of simplicity in machine learning. Models like the GaussianProcessClassifier and RandomForestClassifier performed worse than the more simple LogisticRegression. When strong trends exist in datasets, we need not reinvent the wheel and overcomplicate the methods of classification. In order for the model to be generalizable to new data, it must make decisions based on logical trends, not just based on localized patterns within the training dataset."
  },
  {
    "objectID": "posts/blog-post-2-design/index.html",
    "href": "posts/blog-post-2-design/index.html",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "",
    "text": "Abstract\nWhen prospective borrowers request loans from banks, the bank is faced with a fiscal decision to provide the loan or reject the loan. Should the borrower default on the loan, the bank will likely take a loss. However, should they repay the loan, the bank will turn a profit through interest. This blog post will explore one methodology for an automated decision making process for granting or rejecting loans, using weights derived from a machine learning model. These weights will be used to determine the optimal threshold at which the bank should grant or deny loans, assuming we want to optimize for profit. The fairness of this model will then be examined, with a discussion of whether it is acceptable for certain people to have an easier time accessing credit over others.\n\n\nExploration\nWe will be working with a dataset of borrowers, with a target variable of whether they ended up defaulting on the loan or not. First, let’s take a look at the data:\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\n\n\ndf_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10\n\n\n\n\n\n\n\nIn this dataframe, loan_status is the target variable, which is a 0 if the loan was repaid, and a 1 if the borrower defaulted on the loan. We will therefore need to split this column from the rest of the data. We will also drop the loan_grade column, as that is the bank’s evaluation of how likely the borrower is to repay the loan (which is what we want to figure out ourselves!). Lastly, we will drop any columns with missing data, and convert any qualitative columns to one-hot columns so that our eventual machine learning model will have an easier time with the features.\n\n# Pre-processing\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(df_train[\"loan_status\"]) # for 0, 1, 2 etc. cols\n\ndef prepare_data(df):\n  df = df.dropna()\n  y = le.transform(df[\"loan_status\"])\n  df = df.drop([\"loan_status\"], axis = 1)\n  df = df.drop([\"loan_grade\"], axis = 1)\n  df = pd.get_dummies(df, dtype=int)\n  return df, y\n\nX_train, y_train = prepare_data(df_train)\nX_train\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\nperson_home_ownership_MORTGAGE\nperson_home_ownership_OTHER\nperson_home_ownership_OWN\nperson_home_ownership_RENT\nloan_intent_DEBTCONSOLIDATION\nloan_intent_EDUCATION\nloan_intent_HOMEIMPROVEMENT\nloan_intent_MEDICAL\nloan_intent_PERSONAL\nloan_intent_VENTURE\ncb_person_default_on_file_N\ncb_person_default_on_file_Y\n\n\n\n\n1\n27\n98000\n3.0\n11750\n13.47\n0.12\n6\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n1\n\n\n2\n22\n36996\n5.0\n10000\n7.51\n0.27\n4\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n1\n0\n\n\n3\n24\n26000\n2.0\n1325\n12.87\n0.05\n4\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n1\n0\n\n\n4\n29\n53004\n2.0\n15000\n9.63\n0.28\n10\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n\n\n6\n21\n21700\n2.0\n5500\n14.91\n0.25\n2\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n1\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n26059\n36\n150000\n8.0\n3000\n7.29\n0.02\n17\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n\n\n26060\n23\n48000\n1.0\n4325\n5.42\n0.09\n4\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n1\n0\n\n\n26061\n22\n60000\n0.0\n15000\n11.71\n0.25\n4\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n1\n0\n\n\n26062\n30\n144000\n12.0\n35000\n12.68\n0.24\n8\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n0\n\n\n26063\n25\n60000\n5.0\n21450\n7.29\n0.36\n4\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n1\n0\n\n\n\n\n22907 rows × 19 columns\n\n\n\nNow we are ready to begin exploring the data. It will be useful to have some idea of potential correlations between features before training the model, as this will help us understand the results we obtain. One potential correlation I thought would be interesting to look at is what interest rate the bank offers a borrower as it relates to the length of the borrower’s last employment. Let’s first take a look at the minimum, maximum, and average of these features.\n\nprint(\"min interest rate: \", X_train[\"loan_int_rate\"].min())\nprint(\"max interest rate: \", X_train[\"loan_int_rate\"].max())\nprint(\"mean interest rate: \", X_train[\"loan_int_rate\"].mean())\n\nprint(\"\\n\")\nprint(\"min emp length: \", X_train[\"person_emp_length\"].min())\nprint(\"max emp length: \", X_train[\"person_emp_length\"].max())\nprint(\"mean emp length: \", X_train[\"person_emp_length\"].mean())\n\nmin interest rate:  5.42\nmax interest rate:  23.22\nmean interest rate:  11.03494608634915\n\n\nmin emp length:  0.0\nmax emp length:  123.0\nmean emp length:  4.787357576286724\n\n\nThe minimum interest rate that the bank offered a borrower was 5.42%, the maximum was 23.22%, and the mean falls at around 11%. The minimum last employment length was zero, which makes sense, as some borrowers have likely never worked before. The maximum was 123 years, which is almost certainly an error in the data, especially considering that the average was about 4.75 years. For the sake of simplicity, we will consider last employment lengths from 0 to 20 years, and use a chart to visualize what proportion of different employment length buckets received low interest rates versus high interest rates. The following code block generates Figure 1, which shows this relationship.\n\n# Visualization 1\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nX_train_dv = X_train.copy()\n\n# Define employment length bins\nemp_bins = list(range(0, 24, 3))  # Bins: [0-2, 3-5, 6-8, ..., 18-20]\nemp_labels = [\"0-2\", \"3-5\", \"6-8\", \"9-11\", \"12-14\", \"15-17\", \"18-20\"]\n\n# Categorize employment length into bins\nX_train_dv[\"emp_length_group\"] = pd.cut(X_train_dv[\"person_emp_length\"], bins=emp_bins, labels=emp_labels, right=True, include_lowest=True)\n\n# Categorize interest rates\nX_train_dv[\"rate_category\"] = X_train_dv[\"loan_int_rate\"].apply(lambda x: \"Below 11%\" if x &lt; 11 else \"Above 11%\")\n\n# Create the bar plot\nplt.figure(figsize=(12, 6))\nsns.countplot(\n    data=X_train_dv, \n    x=\"emp_length_group\", \n    hue=\"rate_category\",\n    palette=\"Set1\"\n)\n\n# Customize labels\nplt.xlabel(\"Employment Length (Years)\")\nplt.ylabel(\"Count of Individuals\")\nplt.title(\"Fig 1: Interest Rate Distribution by Employment Length\")\nplt.legend(title=\"Interest Rate Category\")\nplt.xticks(rotation=45)\n\nplt.show()\n\n\n\n\n\n\n\n\nAs we can see, borrowers with last employment lengths of 0-2 years receive above average interest rates more often than borrowers with longer last employment lengths. This suggests that borrowers who have had strong careers are offered more favorable deals than borrowers who have not worked stable for lengthy periods of time. The bank may do this as a form of risk protection, as a higher interest rate will encourage a borrower to repay their loan in a timely manner. However, this may have disparate impacts on people whose last employment length was low, as not having had a long-term job suggests that they may lack savings. Taking on a high-interest loan may be unviable for them, so this distribution may impact some borrowers’ ability to access affordable credit.\n\n# Visualization 2\nplt.figure(figsize=(10, 5))\nsns.countplot(\n    data=df_train, \n    x=\"person_home_ownership\", \n    hue=\"loan_intent\",\n    palette=\"Set2\"\n)\n\nplt.xlabel(\"Home Ownership Status\")\nplt.ylabel(\"Count\")\nplt.title(\"Fig 2: Loan Intent by Home Ownership Status\")\nplt.legend(title=\"Loan Intent\")\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2 plots the relation between loan intent and home ownership status. Renters have the highest rates of education and medical loans, mortgagers have a high rate of education loans as well as personal and debt consolidation loans. Home owners, on the other hand, have disproportionately high rates of venture loans. Venture loans, importantly, are the type of loan most likely to imply that its use is to expand the borrower’s capital even further. Medical loans or debt consolidation loans may be necessary for the borrower’s continued well-being, whereas venture loans are likely optional loans being taken because there is an opportunity for fiscal gain. What this suggests is that home renters or mortgagers may be in a less stable position than homeowners, which means that their loans are more focused on continued survival rather than development of wealth. If it turns out that venture loans are more likely to be repaid than medical loans, for example, this may lead a profit-driven bank to provide fewer essential loans and more investment loans, which can have strong negative impacts on those in need.\n\n# Summary Table\nsummary_table = df_train.groupby(\"cb_person_default_on_file\").aggregate(\n    Mean_Income_USD=(\"person_income\", \"mean\"),\n    Median_Income_USD=(\"person_income\", \"median\"),\n    Mean_Loan_Size_USD=(\"loan_amnt\", \"mean\"),\n    Median_Loan_Size_USD=(\"loan_amnt\", \"median\")\n)\n\n# Display the table\nprint(summary_table)\n\n                           Mean_Income_USD  Median_Income_USD  \\\ncb_person_default_on_file                                       \nN                             65990.726858            55315.0   \nY                             65900.076251            54000.0   \n\n                           Mean_Loan_Size_USD  Median_Loan_Size_USD  \ncb_person_default_on_file                                            \nN                                 9464.480849                8000.0  \nY                                10062.338868                8100.0  \n\n\nThe summary table above displays the mean and median income as well as mean and median loan size, organized by whether the borrowers had a default on file or not. My suspicion before making this table was that borrowers with a default on file may have lower income than borrowers without one, and that borrowers with a default on file may only be approved for smaller loans than borrowers without one. Interestingly, neither of my predicted relationships seem to hold, as mean and median income are very similar across both categories, and mean loan size is actually higher for those with a default on file than those without one. This may suggest that having a default on file is not a significant factor in the bank’s decision making, and that having a default on file is not correlated with income. Therefore, our machine learning model may place relatively little weight on loan size or income.\n\n\nModeling\nNow it is time to train a model to predict whether a borrower will default or not. This will result in a vector of weights \\(\\mathbf{w}\\), which we can then use to decide whether loans should be approved or not.\nTo begin, we’ll need to choose what features we want to use to train the model– for simplicity, we can simply provide the model with all of the features as a starting point, and evaluate its accuracy. In many cases, this approach is effective out of the box, and won’t require any further selection of features.\nThe following code block simply trains a LogisticRegression on all columns of X_train (except loan_grade of course, since it was dropped earlier). The only further transformation we will apply to the data is a logorithmic scale to income, as the wide range of values present in the income column is cumbersome for the LogisticRegression. By scaling it, the LR can converge more efficiently. We will then use 5-fold cross validation to simulate a test of the model, and evaluate the average accuracy. The output will contain some warnings regarding the number of iterations, due to the scale of the data being very large– we could increase the number of iterations to account for this, but this process is a good starting point, and leaving the number of iterations at default maintains a low running time.\n\n# Train an LR using all features except loan grade\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\n\nX_train[\"person_income\"] = np.log1p(X_train[\"person_income\"])  # log scale person income due to wide range\n\nLR = LogisticRegression()\nLR.fit(X_train, y_train)\nscores = cross_val_score(LR, X_train, y_train, cv=5) # Using 5-fold cross validation\n\n/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n\nprint(np.mean(scores))\n\n0.8159947464862315\n\n\nThe model achieved an average accuracy of 81.6%, which is reasonably accurate for this sort of prediction. Embedded in this model is our vector of weights \\(\\mathbf{w}\\), which we can use to compute a borrower score for each borrower in the training set. The LR uses this score to make a prediction on its own, but we will manually use the vector to make our own prediction by determing the cutoff point at which we can maximize profit. For simplicity, I will use the provided mathematical model, where the profits for the case where the loan is repaid and the case where the loan is defaulted are as follows:\n\\[\n\\text{Repaid Profit} = \\text{Loan Amount} \\times (1 + 0.25 \\times \\text{Loan Interest Rate})^{10} - \\text{Loan Amount}\n\\]\n\\[\n\\text{Defaulted 'Profit'} = \\text{Loan Amount} \\times (1 + 0.25 \\times \\text{Loan Interest Rate})^{3} - 1.7 \\times \\text{Loan Amount}\n\\]\nThe following code block uses this mathematical model to determine the optimal threshold at which we should stop accepting loans. It first calculates the score for each borrower, then creates a new dataframe which contains the borrower’s score, the outcome of their loan (whether they repaid it or defaulted on it), and the profit made from their loan in either case as determined by the mathematical model above. It then adds a new column to the dataframe, expected profit, which contains the actual profit made if this loan is accepted based on the outcome of the loan and the profit calculation. Lastly, it calculates the cumulative profit made if we accept all borrowers above every possible threshold, then prints out the threshold at which the maximum cumulative profit is made, as well as what that profit is, and what the profit per borrower is.\n\n# Find the optimal profit threshold based on the predictions made by the LR\nborrower_scores = X_train.values @ LR.coef_.ravel() # ravel method fixes dimensions of coef vector\nborrower_scores = borrower_scores.flatten() # flatten method converts ~20000 * 1 matrix to a 1d array of length ~20000\n\nrepay_profit = X_train[\"loan_amnt\"] * (1 + 0.25*(X_train[\"loan_int_rate\"]/100))**10 - X_train[\"loan_amnt\"]\ndefault_profit = X_train[\"loan_amnt\"] * (1 + 0.25 *(X_train[\"loan_int_rate\"]/100))**3 - 1.7*X_train[\"loan_amnt\"]\n\ndf_scores = pd.DataFrame({\n    \"score\": borrower_scores,\n    \"outcome\": y_train,\n    \"repay_profit\": repay_profit,\n    \"default_profit\": default_profit\n})\n\ndf_scores = df_scores.sort_values(by=\"score\", ascending=False).reset_index(drop=True) # sort so we can test thresholds from top to bottom\n\ndf_scores[\"expected_profit\"] = df_scores[\"outcome\"] * df_scores[\"repay_profit\"] + (1 - df_scores[\"outcome\"]) * df_scores[\"default_profit\"]\ndf_scores = df_scores.sort_values(by=\"score\")\nsorted_scores = df_scores[\"score\"].values\nsorted_profits = df_scores[\"expected_profit\"].values\n\n# Compute cumulative profit in reverse order\ncumulative_profit = np.cumsum(sorted_profits[::-1])[::-1] # cumulative profit across all approved borrowers\nnum_borrowers = np.arange(len(sorted_scores), 0, -1) # total num of borrowers\nprofit_per_borrower = cumulative_profit / num_borrowers  \n\n# Find the threshold that maximizes profit\noptimal_index = np.argmax(cumulative_profit)\noptimal_threshold = sorted_scores[optimal_index]\noptimal_profit = cumulative_profit[optimal_index]\noptimal_PPB = profit_per_borrower[optimal_index]\n\nprint(\"Optimal Threshold: \", optimal_threshold)\nprint(\"Optimal Profit: \", optimal_profit)\nprint(\"Optimal PPB: \", optimal_PPB)\n\nOptimal Threshold:  0.14241452471915317\nOptimal Profit:  3313401.632051698\nOptimal PPB:  2754.2823209074795\n\n\nThis model imputes an optimal score threshold of about 0.14, at which the profit on the training set is calculated to be 3.3 million dollars, with a profit per borrower of $2754.28. The following code block visualizes all possible thresholds as a line, showing clearly where the maxima lies.\n\nplt.figure(figsize=(10, 5))\nplt.plot(sorted_scores, cumulative_profit, label=\"Cumulative Profit\")\nlabel1 = (\"Optimal Threshold: \" + str(float(optimal_threshold)))\nplt.axvline(optimal_threshold, color='r', linestyle='dashed', label=label1)\nlabel2 = (\"Optimal Profit: $\" + str(round(float(optimal_profit), 2)))\nplt.scatter(optimal_threshold, optimal_profit, color='green', zorder=3, s=17, label = label2)\n\nplt.xlabel(\"Borrower Score (Threshold)\")\nplt.xlim(0.0, 0.5)\nplt.ylabel(\"Cumulative Profit (millions USD)\")\nplt.ylim(2000000, 3500000)\nplt.title(\"Optimized Profit vs. Score Threshold\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nEvaluation\nTo properly evaluate this model, we must now test it out on a test data set to ensure that it is not subject to overfitting on the training set. To accomplish this, we will pull the test data and apply the same preparation steps that we took with the training data. We will then apply our calculated optimal threshold, and determine the profit if we accept all borrowers in the test set with scores above the threshold.\n\n# Evaluate on test dataset\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\n\nX_test, y_test = prepare_data(df_test)\nX_test[\"person_income\"] = np.log1p(X_test[\"person_income\"])  # log scale person income due to wide range\n\n\nborrower_scores_test = X_test.values @ LR.coef_.ravel()\nborrower_scores_test = borrower_scores_test.flatten()\n\nthreshold = 0.14241452471915317\nloan_approved = borrower_scores_test &gt;= threshold\nrepay_profit_test = X_test[\"loan_amnt\"] * (1 + 0.25*(X_test[\"loan_int_rate\"]/100))**10 - X_test[\"loan_amnt\"]\ndefault_profit_test = X_test[\"loan_amnt\"] * (1 + 0.25 *(X_test[\"loan_int_rate\"]/100))**3 - 1.7*X_test[\"loan_amnt\"]\n\ndf_scores_test = pd.DataFrame({\n    \"score\": borrower_scores_test,\n    \"outcome\": y_test,\n    \"repay_profit\": repay_profit_test,\n    \"default_profit\": default_profit_test\n})\n\ndf_scores_test[\"expected_profit\"] = df_scores_test[\"outcome\"] * df_scores_test[\"repay_profit\"] + (1 - df_scores_test[\"outcome\"]) * df_scores_test[\"default_profit\"]\n\nprofit_test = np.where(\n    loan_approved,\n    df_scores_test[\"expected_profit\"],\n    0  # 0 change if the loan is denied\n)\n\nrevenue = profit_test[profit_test &gt; 0]\ntotal_profit_test = revenue.sum()\nprofit_per_borrower_test = revenue.mean()\n\nprint(\"Total Profit: \", total_profit_test)\nprint(\"Average PPB: \", profit_per_borrower_test)\nprint(\"Approval Rate:\", (loan_approved.sum() / len(df_test)) * 100, \"%\")\nprint(\"Approved Loans: \", len(revenue))\nprint(\"Total Requested Loans: \", len(profit_test))\n\nTotal Profit:  365142.9799703294\nAverage PPB:  8693.880475484033\nApproval Rate: 0.7365352155899954 %\nApproved Loans:  42\nTotal Requested Loans:  5731\n\n\nAs we can see, the model is profitable on the test set as well, with a profit of $365,142.98, and a profit per borrower of $8693.88. These scores indicate a high level of profitability for the bank, and show that the model generalizes effectively. However, we also see that the approval rate is very low, at 0.74%. Out of 5731 prospective borrowers, only 42 were given loans under this pure capital optimization model. While this may be perfect for the bank in terms of making money, the borrower adds another side to the story.\n\n\nThe Borrower’s Perspective\n\nIs it more difficult for people in certain age groups to access credit under your system?\n\nTo answer this question, let’s take a look at the rows in X_test (the borrowers) that were approved to get a loan based on the optimal threshold I determined. The following line of code prints the ages of each approved borrower. The line after prints the min, max, and average age of the approved borrowers.\n\nprint(X_test[loan_approved &gt; 0][\"person_age\"])\n\n241     23\n276     31\n312     22\n318     23\n339     31\n350     25\n391     23\n437     24\n489     29\n550     22\n650     23\n705     21\n952     27\n1050    30\n1211    21\n1241    27\n1311    22\n1423    27\n1669    25\n2086    23\n2218    23\n2358    22\n2375    22\n2434    24\n3492    28\n3608    23\n3763    21\n3821    21\n3974    23\n4098    23\n4116    24\n4243    22\n4542    22\n4740    28\n4785    21\n4901    23\n4945    27\n4980    23\n5316    22\n5592    22\n5645    27\n5924    28\n6013    27\n6015    26\n6102    21\n6118    24\n6447    22\n6491    27\nName: person_age, dtype: int64\n\n\n\nprint(X_test[loan_approved &gt; 0][\"person_age\"].min())\nprint(X_test[loan_approved &gt; 0][\"person_age\"].max())\nprint(X_test[loan_approved &gt; 0][\"person_age\"].mean())\n\n21\n31\n24.270833333333332\n\n\nAs we can see, the youngest borrower who was approved for a loan was 21 years old, and the oldest one was 31 years old. This is a tight one decade range, which suggests that older borrowers are being unilaterally denied. To see how significant the impact is, let’s take a look at how many prospective borrowers there were that are over the age of 31.\n\nprint((X_test[\"person_age\"] &gt; 31).sum())\n\n1180\n\n\n1180 borrowers above the age of 31 were denied credit. While it is possible (and likely) that factors other than age played a role in determining whether these borrowers would be approved or not, this result still suggests that the model unfairly trends towards younger borrowers.\n\nIs it more difficult for people to get loans in order to pay for medical expenses? How does this compare with the actual rate of default in that group? What about people seeking loans for business ventures or education?\n\nTo answer this question, let’s take a look at the proportion of approval and default for medical loans, education loans, and venture loans. The following code block computes the approval rate and default rate in each category of loan, then displays the results in a summary table.\n\ncategories = [\"loan_intent_MEDICAL\", \"loan_intent_EDUCATION\", \"loan_intent_VENTURE\"]\nloan_approved_dv = df_scores_test[\"score\"] &gt;= optimal_threshold\nsummary_data = []\n\n# Loop through each category and compute metrics\nfor category in categories:\n    # Filter for loans in the current category\n    category_mask = X_test[category] == 1\n\n    # Calculate approval rate and default rate\n    approval_rate = loan_approved_dv.loc[category_mask].mean() * 100  # % of loans approved in current category\n    default_rate = (y_test[category_mask] == 1).mean() * 100  # % of loans that defaulted in current category\n\n    # Append results to summary data\n    summary_data.append([category.replace(\"loan_intent_\", \"\").title(), approval_rate, default_rate])\n\n# Create DataFrame\nsummary_table = pd.DataFrame(summary_data, columns=[\"Loan Intent\", \"Approval Rate (%)\", \"Default Rate (%)\"])\n\n# Display summary table\nprint(summary_table)\n\n  Loan Intent  Approval Rate (%)  Default Rate (%)\n0     Medical           1.118360         28.424977\n1   Education           0.850340         16.751701\n2     Venture           0.414938         14.626556\n\n\nBased on this summary table, borrowers with medical intent have the highest rate of default, at 28%. In spite of this, they have a high approval rate in my model at 1.11%. Education and venture loans have lower rates of default, but lower rates of approval as well. What this suggests is that the LogisticRegression model we trained earlier likely did not consider loan intent as a highly weighted category for prediction, as, if it did, it is likely that fewer medical loans would get approved than venture loans.\n\nHow does a person’s income level impact the ease with which they can access credit under your decision system?\n\nThe following code block prints the minimum, maximum, mean, and median income of borrowers who were approved, then borrowers who were not approved. Because we applied a logarithmic scale to the income column in the process of data preparation, we will go back to the original test dataframe to perform this, with the only transformation being dropping the same rows we dropped in the preparation so that the arrays are the same length.\n\ndf_test = df_test.dropna()\nprint(df_test[loan_approved &gt; 0][\"person_income\"].min())\nprint(df_test[loan_approved &gt; 0][\"person_income\"].max())\nprint(df_test[loan_approved &gt; 0][\"person_income\"].mean())\nprint(df_test[loan_approved &gt; 0][\"person_income\"].median())\n\nprint(\"\\n\")\nprint(df_test[loan_approved == 0][\"person_income\"].min())\nprint(df_test[loan_approved == 0][\"person_income\"].max())\nprint(df_test[loan_approved == 0][\"person_income\"].mean())\nprint(df_test[loan_approved == 0][\"person_income\"].median())\n\n21000\n168000\n58263.395833333336\n52000.0\n\n\n4800\n1782000\n66670.4568009854\n55000.0\n\n\nThe lowest income of an accepted borrower was $21,000 USD, and the maximum was $168,000 USD. The average was $58,263.40 USD, and the median was $52,000 USD. For rejected borrowers, the minimum income was $4,800 USD, the maximum was 1.7 million USD, the average was $66,670.46 USD, and the median was $55,000 USD. What this suggests is that very low income borrowers are unable to access credit, as anyone with under $21,000 USD in yearly income is denied credit. That being said, $21,000 USD per year is achievable with a minimum wage job, so this suggests that the bar for being of enough income to be approved is having a job. This being said, the median incomes suggest that the mean for the denied borrowers may be unusually high due to outliers. The medians are very similar, which suggests that income is not a major factor in accessing credit under this model.\n\n\nDiscussion\nIn this modeling exercise, we found that optimizing profit often leads to very low lending rates for banks. The optimal approval rate for the bank was only 0.73%, which shows that making the most possible money leads to very limited access to credit for borrowers. Interestingly, the model did not discriminate based on loan intent much at all, despite strongly differing rates of default between different intention categories. Age was a seemingly important factor, with no prospective borrower above the age of 31 being granted a loan. It could be argued that using all features to train the LogisticRegression model introduced opportunities for bias in the decisions, as if people 31 years or older, for example, show a high rate of default, they may have more difficulty accessing credit under this automated decision systems. In order for systems to be fair, it is possible that optimizing purely for profit is not ideal, as patterns in data may produce uneven outcomes for borrowers, which is ideally to be avoided.\n\nConsidering that people seeking loans for medical expenses have a high rate of default, is it fair that it is more difficult for them to obtain access to credit?\n\nIn answering this question, I apply the following definition of fairness: In a fair system, all parties have equal chances of succeeding at, or taking advantage of an opportunity.\nUnder this definition, I find that it is technically fair for people seeking loans for medical expenses to have a harder time obtaining access to credit. As long as this increased difficulty is equally true for all people seeking medical loans, and they are subject to the same decision making process, it is technically fair. However, the outcomes could still be unfair even if the decision making process is fair. For example, if part of the decision for whether to give a medical loan is examining the client’s income, this could give some clients an unfair advantage over others depending on their access to high-paying jobs (which depends on their access to education, which depends hugely on where they were born, which they had no control over). I believe that in an ideal, fair world, everyone seeking medical care would have equal access to it, but that is not the world we live in. Given the unfair world we live in, I find that a decision making process that makes it more difficult for people seeking medical loans to access credit is fair."
  },
  {
    "objectID": "posts/blog-post-3-audit/index.html",
    "href": "posts/blog-post-3-audit/index.html",
    "title": "Auditing Bias",
    "section": "",
    "text": "When implementing and applying machine learning models to problems, it is imperative that we understand the potential for disparate impacts on people. For example, if a machine learning model were used by a company in their hiring process, and that model demonstrated bias towards certain races over others, that would yield unfair outcomes. In this blog post, a model will be trained which predicts the employment status of individuals based on census data taken in Tennessee in 2018. The model will then be audited for racial bias, with examinations of the prediction rates across racial categories, and a statistical examination of whether the model is fair or not."
  },
  {
    "objectID": "posts/blog-post-3-audit/index.html#by-group-measures",
    "href": "posts/blog-post-3-audit/index.html#by-group-measures",
    "title": "Auditing Bias",
    "section": "By-group measures",
    "text": "By-group measures\nTo begin, we will re-calculate the accuracy, positive predictive value, and false positive/false negative rates of the model by each sub-group. Due to the sample size being relatively small across groups outside of black or white, we will group all other self-identified race categories into a single group.\n\nprint(\"Accuracy for white respondents:\", (y_hat == y_test)[group_test == 1].mean()*100, \"%\")\nprint(\"Accuracy for black respondents:\", (y_hat == y_test)[group_test == 2].mean()*100, \"%\")\nprint(\"Accuracy for self-identified race respondents:\", (y_hat == y_test)[group_test &gt;= 3].mean()*100, \"%\")\n\nprint(\"\\n\")\n\nprint(\"PPV for white respondents:\", precision_score(y_test[group_test==1], y_hat[group_test==1])*100, \"%\")\nprint(\"PPV for black respondents:\", precision_score(y_test[group_test==2], y_hat[group_test==2])*100, \"%\")\nprint(\"PPV for self-identified race respondents:\", precision_score(y_test[group_test&gt;=3], y_hat[group_test&gt;=3])*100, \"%\")\n\nAccuracy for white respondents: 82.97379211980746 %\nAccuracy for black respondents: 82.17880402605093 %\nAccuracy for self-identified race respondents: 83.30893118594437 %\n\n\nPPV for white respondents: 78.5496866606983 %\nPPV for black respondents: 74.52471482889734 %\nPPV for self-identified race respondents: 77.59740259740259 %\n\n\nAs we can see, the accuracy is relatively similar across all categories, being within 1.2 percentage points of each other. Black respondents are subject to the least accurate predictions, while self-identified race respondents have the most accurate predictions. That said, when it comes to PPV, the numbers are a bit more spread, with a 4 percentage point difference from the most ppv (white respondents) to the least ppv (black respondents). This may suggest that the model over-predicts employment for black respondents, as the case of a positive prediction but a negative outcome is more common for that group based on the PPV. The following code blocks generate the false positive and false negative rates for each racial group.\n\ntn1, fp1, fn1, tp1 = confusion_matrix(y_test[group_test==1], y_hat[group_test==1]).ravel()\nfpr1 = fp1 / (fp1+tn1)\nfnr1 = fn1 / (fn1+tp1)\n\nprint(\"The False Positive Rate for white respondents is\", fpr1*100, \"%\")\nprint(\"The False Negative Rate for white respondents is\", fnr1*100, \"%\")\n\nThe False Positive Rate for white respondents is 19.57836247752901 %\nThe False Negative Rate for white respondents is 13.963522259266522 %\n\n\n\ntn2, fp2, fn2, tp2 = confusion_matrix(y_test[group_test==2], y_hat[group_test==2]).ravel()\nfpr2 = fp2 / (fp2+tn2)\nfnr2 = fn2 / (fn2+tp2)\n\nprint(\"The False Positive Rate for black respondents is\", fpr2*100, \"%\")\nprint(\"The False Negative Rate for black respondents is\", fnr2*100, \"%\")\n\nThe False Positive Rate for black respondents is 20.07992007992008 %\nThe False Negative Rate for black respondents is 14.534883720930234 %\n\n\n\ntn3, fp3, fn3, tp3 = confusion_matrix(y_test[group_test&gt;=3], y_hat[group_test&gt;=3]).ravel()\nfpr3 = fp3 / (fp3+tn3)\nfnr3 = fn3 / (fn3+tp3)\n\nprint(\"The False Positive Rate for self-identified race category respondents is\", fpr3*100, \"%\")\nprint(\"The False Negative Rate for self-identified race category respondents is\", fnr3*100, \"%\")\n\nThe False Positive Rate for self-identified race category respondents is 17.293233082706767 %\nThe False Negative Rate for self-identified race category respondents is 15.845070422535212 %\n\n\nAs implied earlier by the ppv results, black respondents were subject to the highest false positive rate, at over 20%. Self-identified race respondents were subject to the highest false negative rate, at almost 16%. White respondents are subject to the lowest false negative rate, at almost 14%, and self-identified race respondents were subject to the lowest false positive rate, at just over 17%. Overall, the error rates are relatively close, within 3 percentage points of one another, but are disparate across the different groups, which may be unfair at a wide enough scale. The model does satisfy approximate error rate balance, though further calculation would be needed to determine whether the percentage point disparities are significant or not."
  },
  {
    "objectID": "posts/blog-post-3-audit/index.html#bias-measures",
    "href": "posts/blog-post-3-audit/index.html#bias-measures",
    "title": "Auditing Bias",
    "section": "Bias Measures",
    "text": "Bias Measures\nTo formally understand potential bias, we will evaluate the model in terms of two more statistical definitions of fairness. The first is calibration. A scoring system is said to be calibrated if, across all different scores and all different groups, the rates of actual outcomes are the same. In our case, the scoring system is simply a positive or negative prediction. Notably, this means that calibration is effectively equivalent to the second definition of fairness: statistical parity, which asks whether the rate of outcomes is equal across different predictive groups. Because our score is a 0 or 1 prediction, satisfying calibration implies satisfying statistical parity, and vice versa. The following code block generates the employment rate by race, and prediction of the model on the test set. To make this calculation easier, we will create a small dataframe with the three relevant criteria.\n\n# Calibration evaluation\ncalib_df = pd.DataFrame({\n    'group': group_test,    \n    'y_true': y_test,\n    'prediction': y_hat\n})\n\nmean1 = calib_df[(calib_df['group'] == 1) & (calib_df['prediction'] == 0)]['y_true'].mean()\nprint(\"White respondents, negative prediction: \", mean1*100, \"%\")\n\nmean2 = calib_df[(calib_df['group'] == 2) & (calib_df['prediction'] == 0)]['y_true'].mean()\nprint(\"Black respondents, negative predictions: \", mean2*100, \"%\")\n\nmean3 = calib_df[(calib_df['group'] &gt;= 3) & (calib_df['prediction'] == 0)]['y_true'].mean()\nprint(\"Self-identified race respondents, negative predictions: \", mean3*100, \"%\")\n\nprint(\"\\n\")\n\nmean4 = calib_df[(calib_df['group'] == 1) & (calib_df['prediction'] == 1)]['y_true'].mean()\nprint(\"White respondents, positive predictions: \", mean4*100, \"%\")\n\nmean5 = calib_df[(calib_df['group'] == 2) & (calib_df['prediction'] == 1)]['y_true'].mean()\nprint(\"Black respondents, positive predictions: \", mean5*100, \"%\")\n\nmean6 = calib_df[(calib_df['group'] &gt;= 3) & (calib_df['prediction'] == 1)]['y_true'].mean()\nprint(\"Self-identified race respondents, positive predictions: \", mean6*100, \"%\")\n\nWhite respondents, negative prediction:  12.639801171666962 %\nBlack respondents, negative predictions:  11.11111111111111 %\nSelf-identified race respondents, negative predictions:  12.0 %\n\n\nWhite respondents, positive predictions:  78.5496866606983 %\nBlack respondents, positive predictions:  74.52471482889734 %\nSelf-identified race respondents, positive predictions:  77.59740259740259 %\n\n\nAcross race and predictive categories, rates of outcome are relatively similar, with a 1.5% distribution in the case of negative predictions, and a 4% distribution in the case of positive predictions (the positive category is equivalent to ppv). Overall, the model seems decently calibrated, especially in the negative case, though further statistical analysis would be necessary to determine if the 4% gap in the positive case is significant. This implies that the model also reasonably satisfies statistical parity, since, as discussed before, the two are effectively identical in the case of prediction."
  },
  {
    "objectID": "posts/blog-post-3-audit/index.html#fairness",
    "href": "posts/blog-post-3-audit/index.html#fairness",
    "title": "Auditing Bias",
    "section": "Fairness",
    "text": "Fairness\nTo determine how fair the model could be given some adjustments, we will now reproduce Fig. 7 from Chouldecova, which plots false positive rate as a function of false negative rate. The equation for false positive rate as a function of false negative rate also uses the prevalence of employment in each category, as well as the positive predictive value of the model. Here is the equation:\n\\[\nFPR = \\frac{p}{1 - p} \\times \\frac{1 - PPV}{PPV} \\times (1 - FNR)\n\\]\nSpecifically, we will use the prevalence from each category, but we will assume a constant ppv, as we are interested in changing the false negative rate, not the ppv. We will use the lowest ppv across all three groups in each calculation, meaning we will take the ppv for black respondents, as that was found to be the weakest ppv earlier. The following code performs all of these calculations, then plots the relations between potential false positive rate and false negative rate, as well as the true values of false positive rate and false negative rate for each group. You will notice that the only point which falls on its corresponding line is the point for black respondents; this is because we used the ppv which correlates with black respondents.\n\np1 = y_test[group_test==1].mean()    # Prevalence\np2 = y_test[group_test==2].mean()\np3 = y_test[group_test&gt;=3].mean()\n\nmin_ppv = precision_score(y_test[group_test==2], y_hat[group_test==2]) # Lowest observed PPV was for black respondents\n\ndef compute_fpr(fnr, p, ppv):\n    return (p / (1 - p)) * ((1 - ppv) / ppv) * (1 - fnr) # Equation 2.6\n\nfnr_values = np.linspace(0, 1, 100)\n\nfpr_values_1 = compute_fpr(fnr_values, p1, min_ppv)\nfpr_values_2 = compute_fpr(fnr_values, p2, min_ppv)\nfpr_values_3 = compute_fpr(fnr_values, p3, min_ppv)\n\nplt.figure(figsize=(8, 6))\nplt.plot(fnr_values, fpr_values_1, label='FPR vs FNR (1)', color='blue')\nplt.plot(fnr_values, fpr_values_2, label='FPR vs FNR (2)', color='red')\nplt.plot(fnr_values, fpr_values_3, label='FPR vs FNR (3)', color='green')\n\nplt.scatter(fnr1, fpr1, color='blue', zorder=3, s=17, label=\"Actual Rates (group 1)\")\nplt.scatter(fnr2, fpr2, color='red', zorder=3, s=17, label=\"Actual Rates (group 2)\")\nplt.scatter(fnr3, fpr3, color='green', zorder=3, s=17, label=\"Actual Rates (group 3)\")\n\nplt.xlabel('False Negative Rate (FNR)')\nplt.ylabel('False Positive Rate (FPR)')\nplt.title('FPR as a Function of FNR')\nplt.grid(True)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nBased on this chart, if we want the false positive rate to be equal among all groups, we would have to increase the false negative rate of group 2 to around 27-28%, at which point the actual rate for group 2 would have a matching false positive rate with group 3. Seeing as it currently sits at around 14%, this is a substantial increase, almost doubling the false negative rate. For group 1, the increase would be slightly less significant, as it sits at a lower initial false positive rate, though it would still nearly double. While this solution may decrease the disparate outcomes generated by the model, it would come at a substantial cost in accuracy. There may not be an effective means of correcting this bias without introducing substantial error rates for the model."
  },
  {
    "objectID": "posts/blog-post-4-limits/index.html",
    "href": "posts/blog-post-4-limits/index.html",
    "title": "Limits of the Quantitative Approach to Fairness",
    "section": "",
    "text": "In his speech, Narayanan explains that while quantitative methods do have value and potential use cases, they have become overly trusted over time. Specifically, he mentions the common aphorism in statistics that “all models are wrong, but some models are useful” (Narayanan (2022)). Because statisticians are often forced to make the assumption that their model is ‘wrong’, or that its conclusions may be caused by factors which are irrelevant to the question at hand, they often end up simply defaulting to the null hypothesis, which is a term for the way we assume the world works without outside evidence. In the case of many models, this means disregarding quantitative evidence of bias, as the null hypothesis holds that there is no discrimination.\nNarayanan also notes that the formal belief that the fields of statistics and computer science have held in objectivity may be faulty. In any given research paper, many subjective decisions are made which can alter the results (Narayanan (2022)). When we believe that a result is truly objective, it masks the reality that a human performed the given study. The impossibility of objectivity is further illustrated in our attempts to define fairness. Despite years of concerted effort, the aforementioned fields of research have failed to put forth a uniformly accepted definition of fairness (Narayanan (2022)). There may not exist an objective measure of fairness, and our constant pursuit of objectivity may prevent us from applying useful and important definitions of fairness.\nAll of this being said, Narayanan still does believe in quantitative work of some kind, noting in the end of his speech that there is still great value in describing datasets. Instead of creating models which draw conclusions from datasets, we may spend our effort understanding the nature of the datasets themselves. Historically, this work has been deemed merely descriptive, and lacking in original thought, which has devalued it, but Narayanan points out that we can learn a great deal from simple observation (Narayanan (2022)).\nIn some cases, quantitative methods of fairness can be effective tools for social progress. An example study where quantitative methods were used in a beneficial way was Sahin et. al’s study of a predictive model created for psychiatric treatment. The model in question used features such as race, gender, educational background, financial background, and more to predict whether patients at high risk of psychosis would enter psychosis imminently (Sahin et al. (2024)). It is easy to imagine the potential ethical problems posed by such a model if it exhibits bias. If its predictions were used to justify administering treatment, or, more drastically, people being sent to psychiatric wards, then inaccurate results could lead directly to harming patients.\nSahin et. al examined the model’s results for fairness via several quantitative measures, notably including equality of accuracy and predictive parity. Equality of accuracy requires that for all groups, the model makes correct predictions at the same rate (Barocas, Hardt, and Narayanan (2023)). Sahin et. al found that equality of accuracy was satisfied across all groups, with no statistically significant deviations. Predictive parity requires that that across all groups, the true positive rate, i.e. the rate at which patients were predicted to enter psychosis and did, is balanced (Barocas, Hardt, and Narayanan (2023)). While the model did show slightly higher positive predictive value for males overall, no trends reached statistical significance on this front. However, there was one statistically significant source of bias in the model, which was the false positive rate. Sahin et. al found that the false positive rate for patients with lower levels of education was higher than those with higher levels of education (Sahin et al. (2024)).\nDespite the model satisfying some quantiative definitions of fairness, it showed bias in terms of level of education. This quantitative audit is reasonably useful in and of itself, as it provides some level of understanding of the model’s performance and preferences towards certain features. However, just understanding that the model favors certain features over others does not fully answer whether it is a fair model or not. More specifically, it would be useful to have some metric by which we can deem a model “fair enough” for use. As Narayanan points out, there is no objective truth, and true fairness in predictive models may be an impossibility given the layers of bias that are baked into aspects of machine learning. To account for this difficulty, Sahin et. al took their audit one step further.\nBy comparing the results of the model to the predictions of actual clinicians, Sahin et. al were able to evaluate their model relative to a benchmark, which provides a different perspective on fairness. While the model did show bias towards level of education, studies of clinician’s predictions revealed a similar bias (Sahin et al. (2024)). What this suggests is that while the model may be unfair by some quantitative definition, it is statistically no more unfair than the clinicians performing the work currently. Quantitative methods may not be able to perfectly capture the idea of fairness, but by comparing quantitative analysis of a model to quantitative analysis of human prediction, we may get closer to an acceptable definition of fairness. If we accept that potentially biased humans are ultimately responsible for other humans in some situations, then it is possible that potentially biased models could work as effectively.\nHowever, even if we can find metrics by which a model could be “fair enough”, our methods for evaluating fairness may be subject to bias regardless. A model making ‘fair’ predictions within a biased system still lies within a biased system. Further, attempts to correct bias within a biased system may be counterproductive. In a case study on the “Stop, Question, Frisk” policy, Kallus & Zhou examine how bias within datasets can perpetrate models in a way that cannot be corrected. Below is Figure 1 from Kallus & Zhou’s paper, which illustrates how datasets can be subject to bias in their very collection (Kallus and Zhou (2018)).\n\n\n\nKallus & Zhou Fig. 1\n\n\nIn this figure, the “biased decision policy” refers to the fact that when collecting data,humans are ultimately responsible for who is included and who is excluded. We only get to make predictions on the Z = 1 group, which are the included people in a dataset, but there often exists some excluded Z = 0 group. Because of the difficulty of collecting data on all people for whom a given model may be relevant, some (often many) constituents are bound to be left out of the datasets. In this case, even bias audits cannot capture the potential harms caused by incomplete data.\nIn their case study of SQF, for instance, Kallus & Zhou examined how overpolicing of certain precincts in New York City leads to disproportionate data. In terms of D’Ignazio and Klein’s notion of the Matrix of Domination, the disciplinary domain is at work in this scenario, as overpolicing of certain precincts creates a biased data pool from which it is impossible to train fair models (D’ignazio and Klein (2023)). Figure 4 from Kallus & Zhou, shown below, illustrates the problem clearly (Kallus and Zhou (2018)).\n\n\n\nKallus & Zhou Fig. 4\n\n\nIf police target certain districts over other ones, any demographic differences between the highly policed districts and the underpoliced districts will be reflected in the training data. Statisticians are aware of this difficulty, and as a means of correcting it, will often use quantitative methods to correct the prediction rates as a means of enforcing fairness. To account for this, Kallus & Zhou trained a logistic regression model to predict whether a given person is in criminal possession of a weapon, and then corrected the prediction rates according to two different principles: equality of opportunity and equality of odds. Under equality of opportunity, the true positive rate must be identical across all groups, and under equality of odds, both the true positive and false positive rates must be equal across all groups (Barocas, Hardt, and Narayanan (2023)). They then took the adjusted models and applied them to the full dataset of NYC’s population. The results are shown in Table 2 from their paper below (Kallus and Zhou (2018)).\n\n\n\nKallus & Zhou Table 2\n\n\nEven when equality of opportunity or equality of odds is satisfied, the model still shows systematic bias against certain groups when generalized to the true population of NYC. This is because the available training data is inherently biased, and current quantitative methods of enforcing fairness are insufficient for correcting this injustice.\nThe key problem at play in this case is that data is ultimately an abstraction of reality. When we create predictive models, we use features which describe certain attributes of a given person. These features mark an attempt to use quantitative reasoning to measure qualitative aspects of a human. Knowing someone’s highest level of education does not provide direct insight into how their brain operates, or even their level of intelligence; it is only an approximation of status. This approximation is known as a measurement model, and at the heart of this discussion of fairness is the issue of measurement (Jacobs and Wallach (2021)).\nWhen we take a measurement of a given property, we are forced into making assumptions. For example, say we are predicting whether a prospective borrower will default on a bank loan. The pertinent data regarding a given borrower is essentially their socioeconomic status. The term “socioeconomic status”, unfortunately, refers to a wide sweep of factors, ranging from income to cost of living to occupation and more. When we use a feature such as income to approximate socioeconomic status, we abstract away all other relevant factors, which leaves us with an imperfect picture. Jacobs & Wallach contend that these assumptions remain essentially undiscussed in computer science, and pose a significant theoretical problem to fairness in machine learning (Jacobs and Wallach (2021)).\nQuantitative definitions of fairness are subject to similar issues of assumption. Attempting to measure fairness as a mathematical property is an abstraction of our real understanding of fairness in moral terms. Humans have varying understandings of the term “equality of opportunity”, for example, which means that measuring equality of opportunity statistically is not going to be accurate for everyone (Barocas, Hardt, and Narayanan (2023)). When we deem a certain mathematical explanation “fair”, we risk adoption of that definition without a critical examination of how it falls into our intuitive understandings of fairness (Jacobs and Wallach (2021)).\nTo return to Narayanan’s position with all of this in mind, the claim that quantative methods do more harm than good is worthy of careful consideration. As shown by the case studies of Sahin et. al and Kallus & Zhou, quantitative methods may provide some insight as to how models operate across various groups, but answering for whether they are “fair” or not requires further study and intervention. The predominant acceptance of quantitative methods has allowed many unfair models to be justified, when the actual academic philosophical debate on the issue of fairness remains in a state of aporia. Until we can formalize methods for using and creating models which we deem fair within an acceptable standard, quantitative methods of evaluation largely allow for misrepresentations of equality.\n\n\n\n\nReferences\n\nBarocas, Solon, Moritz Hardt, and Arvind Narayanan. 2023. Fairness and Machine Learning: Limitations and Opportunities. Cambridge, Massachusetts: The MIT Press.\n\n\nD’ignazio, Catherine, and Lauren F Klein. 2023. Data Feminism. MIT press.\n\n\nJacobs, Abigail Z., and Hanna Wallach. 2021. “Measurement and Fairness.” In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 375–85. FAccT ’21. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/3442188.3445901.\n\n\nKallus, Nathan, and Angela Zhou. 2018. “Residual Unfairness in Fair Machine Learning from Prejudiced Data.” https://arxiv.org/abs/1806.02887.\n\n\nNarayanan, Arvind. 2022. “The Limits of the Quantitative Approach to Discrimination.” Speech.\n\n\nSahin, Derya et al. 2024. “Algorithmic Fairness in Precision Psychiatry: Analysis of Prediction Models in Individuals at Clinical High Risk for Psychosis.” The British Journal of Psychiatry 224 (2): 55–65. https://doi.org/10.1192/bjp.2023.141."
  },
  {
    "objectID": "posts/blog-post-5-perceptron/index.html",
    "href": "posts/blog-post-5-perceptron/index.html",
    "title": "Implementing the Perceptron Algorithm",
    "section": "",
    "text": "Link to Implementation\nHere is a link to perceptron.py on my Github repository: https://github.com/Noah5503/Noah5503.github.io/blob/main/posts/blog-post-5-perceptron/perceptron.py\n\n\nAbstract\nThe Perceptron algorithm was one of the first examples of a learning algorithm, which iteratively improves by taking small steps towards the optimal solution. Specifically, the Perceptron is capable of separating two classes of data when the data is linearly separable. It accomplishes this by drawing a line, then selecting a random point which is misclassified by that line, and adjusting the line to correct for the misclassification. In this blog post, we will implement the Perceptron algorithm as a class, and test it on several datasets. Notably, we will evaluate its performance on data that is linearly separable, data that is not linearly separable, and multidimensional data outside of visualizable space.\n\n\nHow it works\n\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\nimport torch\n\nMy implementation of the grad function is as follows:\n\ndef grad(self, X, y):\n    scores = self.score(X)\n    y_ = 2*y - 1\n    misclassified = (scores * y_ &lt; 0).float().unsqueeze(1)\n    return torch.sum(misclassified * (y_.unsqueeze(1) * X), dim=0)\n\nThe math of the Perceptron update function is as follows: if a point \\(x_i\\) is correctly classified, then make no update based on that point; if \\(x_i\\) is misclassified, then perform the update \\(w^{(t+1)}\\) = \\(w^{(t)}\\) + \\((2y_i - 1)*x_i\\). In my grad function, the misclassified variable is a \\(n \\times 1\\) matrix which contains 0s for correctly classified points, and 1s for incorrectly classified points. The grad function returns a vector containing the weight changes, which is equal to \\((2y_i - 1)*x_i\\) for each misclassified point (correctly classified points will multiply that term by 0, rendering it null). The torch.sum() function aggregates these changes to each weight in \\(w\\), so the final returned vector contains the cumulative change for all points in the step.\n\n\nExperiments\nI have imported the following functions from Phil’s lecture notes on 03/12/2025. We will use them to generate data for the algorithm, plot that data using matplotlib, and draw the lines derived from the Perceptron algorithm.\n\nfrom matplotlib import pyplot as plt # From Phil's Lecture notes\nplt.style.use('seaborn-v0_8-whitegrid')\n\ntorch.manual_seed(1234)\n\ndef perceptron_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nX, y = perceptron_data(n_points = 300, noise = 0.2)\n\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\nTo begin, we will generate some linearly separable data. When the data is linearly separable, the Perceptron algorithm will converge to a vector of weights, \\(w\\), which describe a line separating the classes of data. We will know if the algorithm is working or not if it is able to converge on a separating line on this first linearly separable dataset. The noise parameter to the perceptron_data function controls how spaced the points are allowed to be within a class. By choosing a low noise value of 0.2 or 0.3, the data will be more likely to be linearly separable.\n\ntorch.manual_seed(1234)\nX, y = perceptron_data(n_points = 50, noise = 0.3)\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, ax)\nplt.plot([-0.5, 1.5], [1.25, -1.0])\nplt.show()\n\n\n\n\n\n\n\n\nThis data is linearly separable, as shown by the blue line which separates the two classes. Therefore, the Perceptron algorithm will be able to converge on a separating line with loss = 0; in other words, it will correctly find a line which separates the two classes. The following code block implements a training loop for the Perceptron, which calculates an update based on 1 point per iteration, repeating until loss = 0. To visualize this process, we will also generate several charts showing the iterations of lines and loss, as well as the point used to make the update.\n\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nfig, axarr = plt.subplots(2, 3, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1\nloss_vec = []\n\nwhile loss &gt; 0:\n    ax = axarr.ravel()[current_ax]\n\n    # save the old value of w for plotting later\n    old_w = torch.clone(p.w)\n\n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value \n\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    local_loss = p.loss(x_i, y_i).item()\n\n    if local_loss &gt; 0:\n        opt.step(x_i, y_i)\n    # if a change was made, plot the old and new decision boundaries\n    # also add the new loss to loss_vec for plotting below\n    if local_loss &gt; 0:\n        plot_perceptron_data(X, y, ax)\n        draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n        loss = p.loss(X, y).item()\n        loss_vec.append(loss)\n        draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n        ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[2*(y[i].item())-1]])\n        # draw_line(w, -10, 10, ax, color = \"black\")\n        ax.set_title(f\"loss = {loss:.3f}\")\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        current_ax += 1\nplt.tight_layout()\n\n\n\n\n\n\n\n\nIn the top left, we see the initial guess, which then shifted to the right in the top-middle chart, as the highlighted brown circle was selected as the misclassified point for the update. This process repeats with smaller and smaller shifts, until the algorithm successfully converges in the bottom left, drawing a line with zero loss. This optimal solution correctly classifies all points, which was possible since the data was linearly separable. Conversely, if the data is not linearly separable, the Perceptron will be unable to converge. To test this, let’s generate some data which is not linearly separable, which we can accomplish by increasing the noise parameter. This will allow data points to be more loosely grouped, with overlap between the groups.\n\nX2, y2 = perceptron_data(n_points = 50, noise=0.6) # Not linearly separable!\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X2, y2, ax)\n\n\n\n\n\n\n\n\nThis data is not linearly separable, as there is no possible straight line which separates the two classes. This means that the Perceptron algorithm will be unable to converge and achieve loss = 0. However, we can set a maximum number of iterations, which will allow it to reach some local minimum of loss. The following code block implements this process, allowing the Perceptron algorithm to iterate for 1000 steps. It will not achieve zero loss on this dataset, but we may still examine its performance.\n\np2 = Perceptron()\nopt2 = PerceptronOptimizer(p2)\np2.loss(X2, y2)\n\nloss2 = 1.0\n\nloss_vec2 = []\n\nn2 = X2.size()[0]\nmaxsteps = 1000\nj = 0\n\nwhile (loss2 &gt; 0) and (j &lt; maxsteps):\n    \n    # not part of the update: just for tracking our progress    \n    loss2 = p2.loss(X2, y2) \n    loss_vec2.append(loss2)\n    \n    # pick a random data point\n    i = torch.randint(n2, size = (1,))\n    x2_i = X2[[i],:]\n    y2_i = y2[i]\n    \n    # perform a perceptron update using the random data point\n    opt2.step(x2_i, y2_i)\n    j += 1\n\nThe model has now iterated for 1000 steps. To show its progress over the training period, the following code block plots the loss values at each update.\n\nplt.plot(loss_vec2, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec2)), loss_vec2, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nWhen the data is not linearly separable, the algorithm cannot converge, but we also see that it does not necessarily improve at each step in the training period. This is because at each step, there will exist some set of misclassified points. If the algorithm chooses to update based on a point that is widely misclassified, it will overadjust and make the loss worse. This being said, it still ended up with a relatively low loss value:\n\nprint(loss_vec2[-1])\n\ntensor(0.0600)\n\n\nThe final loss is 0.06, which suggests very few misclassified points. Let’s plot the data and the final decision line achieved by the Perceptron.\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X2, y2, ax)\ndraw_line(p2.w, -1, 2, ax, color = \"black\")\n\n\n\n\n\n\n\n\nOn visual inspection, there appear to be only three misclassified points: two orange circles and one blue square. Despite being unable to converge, this decision line is fairly accurate.\nThe Perceptron can also work on data with more than two dimensions. To test this, we will generate a dataset with five dimensions. We will not be able to visualize the data or the decision line created by the algorithm, but we can still plot the loss over time, which will show the algorithm’s progress. The following code blocks implement the same process of data generation, training iteration, and plotting the loss over time as seen previously. Notably, we generate data with noise = 0.2 to promote linear separability.\n\nX3, y3 = perceptron_data(n_points = 50, noise = 0.2, p_dims = 5)\n\n\np3 = Perceptron()\nopt3 = PerceptronOptimizer(p3)\np3.loss(X3, y3)\n\nloss3 = 1.0\n\nloss_vec3 = []\n\nn3 = X3.size()[0]\nmaxsteps = 1000\nj = 0\n\nwhile (loss3 &gt; 0) and (j &lt; maxsteps):\n    \n    # not part of the update: just for tracking our progress    \n    loss3 = p3.loss(X3, y3) \n    loss_vec3.append(loss3)\n    \n    # pick a random data point\n    i = torch.randint(n3, size = (1,))\n    x3_i = X3[[i],:]\n    y3_i = y3[i]\n    \n    # perform a perceptron update using the random data point\n    opt3.step(x3_i, y3_i)\n    j += 1\n\n\nplt.plot(loss_vec3, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec3)), loss_vec3, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nGiven this chart, the Perceptron was able to converge to loss = 0, even on five dimensional data. On the 36th iteration, the loss dropped from 0.06 to 0.00. This suggests that the dataset must have been linearly separable, as zero loss would not be possible otherwise.\n\n\nDiscussion\nTo determine the runtime of a single iteration of the Perceptron algorithm, let’s take a look at the step() method of the PerceptronOptimizer class.\n\ndef step(self, X, y):\n    current_loss = self.model.loss(X, y)\n    gradient = self.model.grad(X, y)\n    self.model.w += gradient\n    return current_loss\n\nThere are three lines of computation in this function. Line 3 adds the result of Perceptron.grad() to the weight vector w, which is a \\(p \\times 1\\) vector. Performing this addition, therefore, requires \\(p\\) operations, so this line is \\(O(p)\\), where \\(p\\) is the number of dimensions in the dataset. Lines 1 and 2 both involve computing the current loss, which is the number of misclassified points. This amounts to performing the matrix multiplication X@w and checking whether the results are correctly classified or not. The running time of this operation is \\(O(n \\times p) + O(n)\\), as the matrix multiplication takes \\(n \\times p\\) steps, and checking the results takes \\(n\\) steps. Therefore, the running time of the entire step() function is \\(O(n \\times p) + O(n) + O(p)\\), which is the same as \\(O(n \\times p)\\).\nWhile the Perceptron algorithm may be dated and relatively low in power, it still marks an important concept in machine learning, which is using iterative learning approaches to optimize some value. By updating the decision line based on a misclassified point, the simple Perceptron model is able to converge to a correct classification line on linearly separable data, even with large numbers of dimensions. More powerful algorithms such as logistic regressions build on this approach by iteratively optimizing convex loss functions, which allow for more powerful classifications. Implementing more complex learning algorithms will be a focus in future work, but the Perceptron serves as an easy to understand starting point."
  },
  {
    "objectID": "posts/blog-post-6-descent/index.html",
    "href": "posts/blog-post-6-descent/index.html",
    "title": "Overfitting, Overparameterization, and Double Descent",
    "section": "",
    "text": "Abstract\nThe classical view of overfitting posits that increasing the number of features used for training beyond the number of samples will result in very low training error, but poor generalization, as the model perfectly interpolates the training data. However, the rise of deep learning has challenged this view, as using large numbers of features has led to even stronger performances on test data than using features below the interpolation point. In this blog post, we use a linear regression algorithm along with random sigmoidal feature maps to demonstrate the phenomenon of “double descent”, where accuracy improves when we pass the interpolation point. The optimal result in terms of mean squared error loss is achieved when using a quantity of features far beyond the interpolation point, showing that using more features than data points does not always lead to overfitting.\n\n\nBackground\nThe standard formula for the optimal weight vector in unregularized least-squares linear regression is as follows:\n\\(\\hat{w} = \\arg\\min_{w} \\|Xw - y\\|^2\\)\nWhere X is an \\({n} \\times {p}\\) matrix. This equation has the following closed form solution when \\({n} &gt; {p}\\):\n\\(\\hat{w} = (X^\\top X)^{-1} X^\\top y\\)\nIf \\({p} &gt; {n}\\), this no longer works because of matrix invertibility. \\({X}\\) is \\({n} \\times {p}\\), and \\({X^\\top}\\) is \\({p} \\times {n}\\). Multiplying these matrices (the \\({X^\\top X}\\) term) results in a \\({p} \\times {p}\\) matrix. For a matrix to be invertible, it must be full rank, meaning that all rows of the matrix must be linearly independent. If \\({X}\\) has \\({n}\\) rows, its maximum rank is \\({n}\\), since there are only \\({n}\\) rows. Any matrix built from multiplying \\({X}\\) by another matrix will be constrained to a maximum rank of \\({n}\\), since it comes from a matrix with rank &lt;= \\({n}\\). Therefore, \\({X^\\top X}\\) will generate a \\({p} \\times {p}\\) matrix with rank &lt;= \\({n}\\), so it will not be invertible, making the solution fail.\nTo account for this, we will use a different optimization function in our model, which does not fail in the case where \\({p}\\) &gt; \\({n}\\).\n\n\nModel Implementation\nWe will be implementing a linear regression model for this demonstration. We will use the previously implemented LinearModel class as a base.\n\nimport torch\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\nclass LinearModel:\n\n    def __init__(self):\n        self.w = None \n\n    def score(self, X):\n        if self.w is None: \n            self.w = torch.rand((X.size()[1]))\n        return X @ self.w \n\n    def predict(self, X):\n        scores = self.score(X)\n        return (scores &gt;= 0)\n\nNow we can create the MyLinearRegression model, which will inherit from LinearModel, with an identical predict method, and a loss function which builds on the predict method. We will use the mean-squared error loss function here.\n\nclass MyLinearRegression(LinearModel):\n\n    def predict(self, X):\n        return X@self.w\n\n    def loss(self, X, y):\n        scores = self.predict(X)\n        return torch.mean((scores -y) ** 2)\n\nLastly, we will implement the optimizer class. Here is where we will use a different optimization function. Instead of using the optimal mean-squared error closed form solution, we will use the Moore-Penrose pseudoinverse of \\({X}\\). The advantage here is that the Moore-Penrose pseudoinverse function does not fail when \\({p}\\) &gt; \\({n}\\), meaning we can potentially achieve higher accuracy by using more features than data points.\n\nclass OverParameterizedLinearRegressionOptimizer:\n\n    def __init__(self, model):\n        self.model = model\n\n    def fit(self, X, y):\n        self.model.w = torch.linalg.pinv(X)@y\n\nIn the experiments on this model, I will use the following functions imported from Phil’s notes to generate random sigmoidal feature maps. This will allow us to vary the number of features used in training, so we can observe the change as we pass the interpolation point.\n\ndef sig(x): \n    return 1/(1+torch.exp(-x))\n\ndef square(x): \n    return x**2\n\nclass RandomFeatures:\n    \"\"\"\n    Random sigmoidal feature map. This feature map must be \"fit\" before use, like this: \n\n    phi = RandomFeatures(n_features = 10)\n    phi.fit(X_train)\n    X_train_phi = phi.transform(X_train)\n    X_test_phi = phi.transform(X_test)\n\n    model.fit(X_train_phi, y_train)\n    model.score(X_test_phi, y_test)\n\n    It is important to fit the feature map once on the training set and zero times on the test set. \n    \"\"\"\n\n    def __init__(self, n_features, activation = sig):\n        self.n_features = n_features\n        self.u = None\n        self.b = None\n        self.activation = activation\n\n    def fit(self, X):\n        self.u = torch.randn((X.size()[1], self.n_features), dtype = torch.float64)\n        self.b = torch.rand((self.n_features), dtype = torch.float64) \n\n    def transform(self, X):\n        return self.activation(X @ self.u + self.b)\n\n\n\nExperiments\nWe will begin with a simple problem just to be certain that the class is working as intended. The following code block generates some simple data which we can use to test our model.\n\nX = torch.tensor(np.linspace(-3, 3, 100).reshape(-1, 1), dtype = torch.float64)\ny = X**4 - 4*X + torch.normal(0, 5, size=X.shape)\n\nplt.scatter(X, y, color='darkgrey', label='Data')\nplt.show()\n\n\n\n\n\n\n\n\nNow we will apply a feature map with 100 features, and fit the dataset to our linear regression model. We will then plot the line of best fit predicted by the model, to see if it looks within reason.\n\nphi = RandomFeatures(n_features=100, activation=sig)\nphi.fit(X)\nX_features = phi.transform(X)\n\nLR = MyLinearRegression()\nopt = OverParameterizedLinearRegressionOptimizer(LR)\nopt.fit(X_features, y)\n\ny_pred = LR.predict(X_features)\nloss = LR.loss(X_features, y)\n\n\nX_np = X.numpy()\ny_np = y.numpy()\ny_pred_np = y_pred.detach().numpy()\n\nplt.figure(figsize=(8,5))\nplt.plot(X_np, y_np, 'b.', label='True data')\nplt.plot(X_np, y_pred_np, 'r-', label='Predictions')\nplt.legend()\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('Random Features + Linear Regression Fit')\nplt.show()\n\n\n\n\n\n\n\n\nThe model has made a reasonable predicting line, so it seems to be working well despite the different optimization function. We can now move on to demonstrating perfect interpolation and double descent.\n\n\nImage Corruption Dataset\nTo explore this phenomenon, we will work with a set of grayscale images. The following code block (taken from Phil’s notes) shows a sample image.\n\nfrom sklearn.datasets import load_sample_images\nfrom scipy.ndimage import zoom\n\ndataset = load_sample_images()     \nX = dataset.images[1]\nX = zoom(X,.2) #decimate resolution\nX = X.sum(axis = 2)\nX = X.max() - X \nX = X / X.max()\nflower = torch.tensor(X, dtype = torch.float64)\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\nax.imshow(flower, cmap='Greys')\noff = ax.axis(\"off\")\n\n\n\n\n\n\n\n\nFor our machine learning model, we will add a random, variable amount of corruption to each image in the dataset, represented by blocks of gray pixels. The regression task will be to predict the number of corruptions in the image using only the image itself. The following code block (also taken from Phil’s notes) contains the function which adds corruption to the images. We will then generate the dataset, placing variable amounts of corruption in each image, and then flatten each image into a torch tensor. Each entry in the torch tensor represents a single pixel in the image.\n\ndef corrupted_image(im, mean_patches = 5): \n    n_pixels = im.size()\n    num_pixels_to_corrupt = torch.round(mean_patches*torch.rand(1))\n    num_added = 0\n\n    X = im.clone()\n\n    for _ in torch.arange(num_pixels_to_corrupt.item()): \n        \n        try: \n            x = torch.randint(0, n_pixels[0], (2,))\n\n            x = torch.randint(0, n_pixels[0], (1,))\n            y = torch.randint(0, n_pixels[1], (1,))\n\n            s = torch.randint(5, 10, (1,))\n            \n            patch = torch.zeros((s.item(), s.item()), dtype = torch.float64) + 0.5\n\n            # place patch in base image X\n            X[x:x+s.item(), y:y+s.item()] = patch\n            num_added += 1\n\n            \n        except: \n            pass\n\n    return X, num_added\n\n\nX, y = corrupted_image(flower, mean_patches = 50)\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\nax.imshow(X.numpy(), vmin = 0, vmax = 1, cmap='Greys')\nax.set(title = f\"Corrupted Image: {y} patches\")\noff = plt.gca().axis(\"off\")\n\n\n\n\n\n\n\n\n\nn_samples = 200\n\nX = torch.zeros((n_samples, flower.size()[0], flower.size()[1]), dtype = torch.float64)\ny = torch.zeros(n_samples, dtype = torch.float64)\nfor i in range(n_samples): \n    X[i], y[i] = corrupted_image(flower, mean_patches = 100)\n\nAs a final preparatory step, we will also perform a train test split so we can see how the interpolation phenomenon operates on training and testing data.\n\nfrom sklearn.model_selection import train_test_split\n\nX = X.reshape(n_samples, -1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n\nNow we have a working dataset, and can fit our model. To accomplish this, we use an iterative approach, applying a feature map to the dataset at each step, with a slowly increasing number of features. I found that using a range from 0 to 300 in steps of 2 features worked best for visualization purposes. At each iteration, we will apply the feature map, though we only call phi.fit on the training dataset, so the testing dataset remains untouched. We then fit the linear regression, compute the loss, and store the result in a list so we can visualize the change as we vary the number of features.\n\nfeature_counts = list(range(0, 301, 2)) # try feature counts from 0 to 300 (steps of 2 for graph clarity)\nlosses_train = []\nlosses_test = []\n\nfor n_features in feature_counts: \n    phi = RandomFeatures(n_features=n_features, activation=square)\n    phi.fit(X_train)\n    X_train_phi = phi.transform(X_train)\n    X_test_phi = phi.transform(X_test)\n\n    LR = MyLinearRegression()\n    opt = OverParameterizedLinearRegressionOptimizer(LR)\n    opt.fit(X_train_phi, y_train)\n\n    loss_train = LR.loss(X_train_phi, y_train).item() # .item converts to float\n    loss_test = LR.loss(X_test_phi, y_test).item()\n    losses_train.append(loss_train)\n    losses_test.append(loss_test)\n\nThe following code block plots the mean-squared loss values as the number of features used increases. The dashed red line is at n_features = 100, which is the interpolation point, where the number of features used surpasses the number of images in the dataset. The minimum loss achieved is also highlighted in green. Also, I used a logarithmic scale to make the visualization more clear, but note that this means the loss values shown on the plots are not exact.\n\nplt.figure(figsize=(8, 5))\nplt.plot(feature_counts, losses_train, marker='o')\n\nplt.yscale('log')\nplt.axvline(x=100, color='red', linestyle='--', label='x = 100')\n\nmin_idx = torch.tensor(losses_train).argmin().item()\nmin_features = feature_counts[min_idx]\nmin_loss = losses_train[min_idx]\nplt.scatter(min_features, min_loss, color='green', s=70, zorder=5, label='Min Loss')\n\nplt.xlabel(\"Number of Random Features\")\nplt.ylabel(\"Mean Squared Error (training)\")\nplt.title(\"MSE vs. Number of Features\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nAs we can see, there is a dramatic drop in loss at the interpolation point, and the optimal training loss was achieved around n_features = 120. To see whether the model fell into overfitting or not, we can now examine the test dataset.\n\nplt.figure(figsize=(8, 5))\nplt.plot(feature_counts, losses_test, marker='o')\n\nplt.yscale('log')\nplt.axvline(x=100, color='red', linestyle='--', label='x = 100')\n\nmin_idx = torch.tensor(losses_test).argmin().item()\nmin_features = feature_counts[min_idx]\nmin_loss = losses_test[min_idx]\nplt.scatter(min_features, min_loss, color='green', s=70, zorder=5, label='Min Loss')\n\nplt.xlabel(\"Number of Random Features\")\nplt.ylabel(\"Mean Squared Error (testing)\")\nplt.title(\"MSE vs. Number of Features\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nprint(\"The minimum loss was\", min_loss, \"achieved using\", min_features, \"features.\")\n\nThe minimum loss was 184.91291376461538 achieved using 292 features.\n\n\nAt the interpolation point, the mean-squared loss increases greatly on test data. However, as we use more and more features, the performance actually improves, with the optimal loss being achieved at n_features = 292. This is the phenomenon of double descent, where we can achieve better testing performance by using greatly more features than the number of datapoints available, which stands in direct contrast to the classical view of overfitting. This result is what powers deep learning models, which are now overparameterized to extreme extents.\n\n\nDiscussion\nIn this blog post, we showed how using certain optimization functions can allow for double descent, which enables the use of overparameterization as a means of improving training performance. Despite the optimal closed-form solution for mean-squared error not working with overparameterization, using the Moore-Penrose pseudoinverse allows overparameterization, which was demonstrated to be highly beneficial. The loss values before the interpolation point were much larger than the loss values after the interpolation point. This has important implications for machine learning, as if we can improve performance by providing more features, then improving the scale of data available to train on will also lead to improved model performance. We also do not have to worry as much about overfitting, which has traditionally been a key challenge in developing highly tuned models."
  },
  {
    "objectID": "posts/blog-post-7-logistic/index.html",
    "href": "posts/blog-post-7-logistic/index.html",
    "title": "Implementing Logistic Regression",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer\n\n\nLink to implementation\nHere is a link to logistic.py on my repository: https://github.com/Noah5503/Noah5503.github.io/blob/main/posts/blog-post-7-logistic/logistic.py\n\n\nAbstract\n\n\nExperiments\n\n\nDiscussion"
  }
]